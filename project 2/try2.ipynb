{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "# Miscellaneous\n",
    "import copy\n",
    "from enum import auto, Enum, unique\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact\n",
    "\n",
    "import wandb\n",
    "import utils\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FOLDER = os.path.join('data', 'Forehead')\n",
    "TRAIN_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'train', 'images')\n",
    "TRAIN_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'train', 'labels')\n",
    "TRAIN_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'train', 'masks')\n",
    "VAL_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'val', 'images')\n",
    "VAL_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'val', 'labels')\n",
    "VAL_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'val', 'masks')\n",
    "TEST_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'test', 'images')\n",
    "TEST_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'test', 'labels')\n",
    "TEST_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'test', 'masks')\n",
    "key_to_image_folder = {\n",
    "    'train': {'images': TRAIN_IMAGES_FOLDER, 'labels': TRAIN_LABELS_FOLDER,'masks': TRAIN_MASKS_FOLDER},\n",
    "    'val': {'images': VAL_IMAGES_FOLDER, 'labels': VAL_LABELS_FOLDER,'masks': VAL_MASKS_FOLDER},\n",
    "    'test': {'images': TEST_IMAGES_FOLDER, 'labels': TEST_LABELS_FOLDER,'masks': TEST_MASKS_FOLDER}, \n",
    "}\n",
    "\n",
    "\n",
    "T_IMG_SIZE = (480, 640, 3)\n",
    "\n",
    "TENSOR_BOARD_BASE   = 'TB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "# numSamplesTrain = 30_000\n",
    "# numSamplesVal   = 10_000\n",
    "# boxFormat       = BBoxFormat.YOLO\n",
    "numCls          = 2 #<! Number of classes\n",
    "# maxObj          = 3\n",
    "\n",
    "# Model\n",
    "# gridSize = 5 #<! The gris is (gridSize x gridSize) \n",
    "\n",
    "# Training\n",
    "batchSize   = 4\n",
    "numWorkers  = 2 #<! Number of workers\n",
    "numEpochs   = 2\n",
    "λ = 20.0 #<! Localization Loss\n",
    "ϵ = 0.1 #<! Label Smoothing\n",
    "\n",
    "# Visualization\n",
    "# numImg = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary functions\n",
    "\n",
    "def only_jpg_files(files: List) -> List[str]:\n",
    "    return [item for item in files if item.endswith('jpg')]\n",
    "\n",
    "def get_bbox(filename: str, dirname: str) -> List[float]:\n",
    "    json_file = os.path.join(dirname, f'{filename}.json')\n",
    "    with open(json_file, 'r') as f:\n",
    "        bbox = json.load(f)['bbox']\n",
    "    return bbox\n",
    "\n",
    "def get_label(filename: str, dirname: str) -> List[int]:\n",
    "    json_file = os.path.join(dirname, f'{filename}.json')\n",
    "    with open(json_file, 'r') as f:\n",
    "        label = json.load(f)['class']\n",
    "    return label\n",
    "\n",
    "def build_dataset(batchsz,dsTrain,dsVal,dstest):\n",
    "\n",
    "    # ADD NUM_WORKERS ETC' when moving to gpu\n",
    "    train = DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchsz, collate_fn=utils.collate_fn, num_workers = numWorkers, drop_last = True, persistent_workers = True)\n",
    "    val   = DataLoader(dsVal, shuffle = False, batch_size = 2 * batchsz, collate_fn=utils.collate_fn, num_workers = numWorkers, persistent_workers = True)\n",
    "    test   = DataLoader(dstest, shuffle = False, batch_size = 2 * batchsz, collate_fn=utils.collate_fn, num_workers = numWorkers, persistent_workers = True)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def build_network(num_classes, typ):\n",
    "    if typ=='Faster RCNN':\n",
    "        weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "        oModel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "        # get number of input features for the classifier\n",
    "        in_features = oModel.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        oModel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    elif typ=='Mask RCNN':\n",
    "        weights = MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "        oModel = maskrcnn_resnet50_fpn(weights=weights)\n",
    "        # get number of input features for the classifier\n",
    "        in_features = oModel.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        oModel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        # now get the number of input features for the mask classifier\n",
    "        in_features_mask = oModel.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        # and replace the mask predictor with a new one\n",
    "        oModel.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer,num_classes)\n",
    "\n",
    "    return oModel\n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9,weight_decay=0.0005)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate,betas = (0.9, 0.99), weight_decay = 2e-4)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "using Faster R-CNN with a Resnet50 Back bone with weights from a model trained on COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "# This can be changes to different weights/Model\n",
    "# suitable models can be found here https://pytorch.org/vision/stable/models.html#object-detection or\n",
    "# https://pytorch.org/vision/stable/models.html#instance-segmentation\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "oModel_org = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                             Kernel Shape              Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "FasterRCNN                                         --                        [0, 4]                    --\n",
       "├─GeneralizedRCNNTransform: 1-1                    --                        [10, 3, 1088, 800]        --\n",
       "├─BackboneWithFPN: 1-2                             --                        [10, 256, 17, 13]         --\n",
       "│    └─IntermediateLayerGetter: 2-1                --                        [10, 2048, 34, 25]        --\n",
       "│    │    └─Conv2d: 3-1                            [7, 7]                    [10, 64, 544, 400]        (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       --                        [10, 64, 544, 400]        (128)\n",
       "│    │    └─ReLU: 3-3                              --                        [10, 64, 544, 400]        --\n",
       "│    │    └─MaxPool2d: 3-4                         3                         [10, 64, 272, 200]        --\n",
       "│    │    └─Sequential: 3-5                        --                        [10, 256, 272, 200]       (215,808)\n",
       "│    │    └─Sequential: 3-6                        --                        [10, 512, 136, 100]       1,219,584\n",
       "│    │    └─Sequential: 3-7                        --                        [10, 1024, 68, 50]        7,098,368\n",
       "│    │    └─Sequential: 3-8                        --                        [10, 2048, 34, 25]        14,964,736\n",
       "│    └─FeaturePyramidNetwork: 2-2                  --                        [10, 256, 17, 13]         --\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-17                 --                        [10, 256, 272, 200]       --\n",
       "├─RegionProposalNetwork: 1-3                       --                        [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                --                        [10, 3, 272, 200]         --\n",
       "│    │    └─Sequential: 3-18                       --                        [10, 256, 272, 200]       1,180,160\n",
       "│    │    └─Conv2d: 3-19                           [1, 1]                    [10, 3, 272, 200]         771\n",
       "│    │    └─Conv2d: 3-20                           [1, 1]                    [10, 12, 272, 200]        3,084\n",
       "│    │    └─Sequential: 3-21                       --                        [10, 256, 136, 100]       (recursive)\n",
       "│    │    └─Conv2d: 3-22                           [1, 1]                    [10, 3, 136, 100]         (recursive)\n",
       "│    │    └─Conv2d: 3-23                           [1, 1]                    [10, 12, 136, 100]        (recursive)\n",
       "│    │    └─Sequential: 3-24                       --                        [10, 256, 68, 50]         (recursive)\n",
       "│    │    └─Conv2d: 3-25                           [1, 1]                    [10, 3, 68, 50]           (recursive)\n",
       "│    │    └─Conv2d: 3-26                           [1, 1]                    [10, 12, 68, 50]          (recursive)\n",
       "│    │    └─Sequential: 3-27                       --                        [10, 256, 34, 25]         (recursive)\n",
       "│    │    └─Conv2d: 3-28                           [1, 1]                    [10, 3, 34, 25]           (recursive)\n",
       "│    │    └─Conv2d: 3-29                           [1, 1]                    [10, 12, 34, 25]          (recursive)\n",
       "│    │    └─Sequential: 3-30                       --                        [10, 256, 17, 13]         (recursive)\n",
       "│    │    └─Conv2d: 3-31                           [1, 1]                    [10, 3, 17, 13]           (recursive)\n",
       "│    │    └─Conv2d: 3-32                           [1, 1]                    [10, 12, 17, 13]          (recursive)\n",
       "│    └─AnchorGenerator: 2-4                        --                        [217413, 4]               --\n",
       "├─RoIHeads: 1-4                                    --                        [0, 4]                    --\n",
       "│    └─MultiScaleRoIAlign: 2-5                     --                        [10000, 256, 7, 7]        --\n",
       "│    └─FastRCNNConvFCHead: 2-6                     --                        [10000, 1024]             --\n",
       "│    │    └─Conv2dNormActivation: 3-33             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-34             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-35             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-36             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Flatten: 3-37                          --                        [10000, 12544]            --\n",
       "│    │    └─Linear: 3-38                           --                        [10000, 1024]             12,846,080\n",
       "│    │    └─ReLU: 3-39                             --                        [10000, 1024]             --\n",
       "│    └─FastRCNNPredictor: 2-7                      --                        [10000, 91]               --\n",
       "│    │    └─Linear: 3-40                           --                        [10000, 91]               93,275\n",
       "│    │    └─Linear: 3-41                           --                        [10000, 364]              373,100\n",
       "=============================================================================================================================\n",
       "Total params: 43,712,278\n",
       "Trainable params: 43,486,934\n",
       "Non-trainable params: 225,344\n",
       "Total mult-adds (Units.TERABYTES): 3.35\n",
       "=============================================================================================================================\n",
       "Input size (MB): 36.86\n",
       "Forward/backward pass size (MB): 47967.55\n",
       "Params size (MB): 174.85\n",
       "Estimated Total Size (MB): 48179.27\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model as is \n",
    "\n",
    "torchinfo.summary(oModel_org, (10, 3, 640, 480), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "# This only changes the head\n",
    "\n",
    "# num_classes = 2  # 1 class (person) + background\n",
    "# build a network\n",
    "NNType='Faster RCNN'\n",
    "oModel=build_network(numCls,NNType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                             Kernel Shape              Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "FasterRCNN                                         --                        [0, 4]                    --\n",
       "├─GeneralizedRCNNTransform: 1-1                    --                        [10, 3, 1088, 800]        --\n",
       "├─BackboneWithFPN: 1-2                             --                        [10, 256, 17, 13]         --\n",
       "│    └─IntermediateLayerGetter: 2-1                --                        [10, 2048, 34, 25]        --\n",
       "│    │    └─Conv2d: 3-1                            [7, 7]                    [10, 64, 544, 400]        (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       --                        [10, 64, 544, 400]        (128)\n",
       "│    │    └─ReLU: 3-3                              --                        [10, 64, 544, 400]        --\n",
       "│    │    └─MaxPool2d: 3-4                         3                         [10, 64, 272, 200]        --\n",
       "│    │    └─Sequential: 3-5                        --                        [10, 256, 272, 200]       (215,808)\n",
       "│    │    └─Sequential: 3-6                        --                        [10, 512, 136, 100]       1,219,584\n",
       "│    │    └─Sequential: 3-7                        --                        [10, 1024, 68, 50]        7,098,368\n",
       "│    │    └─Sequential: 3-8                        --                        [10, 2048, 34, 25]        14,964,736\n",
       "│    └─FeaturePyramidNetwork: 2-2                  --                        [10, 256, 17, 13]         --\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-17                 --                        [10, 256, 272, 200]       --\n",
       "├─RegionProposalNetwork: 1-3                       --                        [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                --                        [10, 3, 272, 200]         --\n",
       "│    │    └─Sequential: 3-18                       --                        [10, 256, 272, 200]       1,180,160\n",
       "│    │    └─Conv2d: 3-19                           [1, 1]                    [10, 3, 272, 200]         771\n",
       "│    │    └─Conv2d: 3-20                           [1, 1]                    [10, 12, 272, 200]        3,084\n",
       "│    │    └─Sequential: 3-21                       --                        [10, 256, 136, 100]       (recursive)\n",
       "│    │    └─Conv2d: 3-22                           [1, 1]                    [10, 3, 136, 100]         (recursive)\n",
       "│    │    └─Conv2d: 3-23                           [1, 1]                    [10, 12, 136, 100]        (recursive)\n",
       "│    │    └─Sequential: 3-24                       --                        [10, 256, 68, 50]         (recursive)\n",
       "│    │    └─Conv2d: 3-25                           [1, 1]                    [10, 3, 68, 50]           (recursive)\n",
       "│    │    └─Conv2d: 3-26                           [1, 1]                    [10, 12, 68, 50]          (recursive)\n",
       "│    │    └─Sequential: 3-27                       --                        [10, 256, 34, 25]         (recursive)\n",
       "│    │    └─Conv2d: 3-28                           [1, 1]                    [10, 3, 34, 25]           (recursive)\n",
       "│    │    └─Conv2d: 3-29                           [1, 1]                    [10, 12, 34, 25]          (recursive)\n",
       "│    │    └─Sequential: 3-30                       --                        [10, 256, 17, 13]         (recursive)\n",
       "│    │    └─Conv2d: 3-31                           [1, 1]                    [10, 3, 17, 13]           (recursive)\n",
       "│    │    └─Conv2d: 3-32                           [1, 1]                    [10, 12, 17, 13]          (recursive)\n",
       "│    └─AnchorGenerator: 2-4                        --                        [217413, 4]               --\n",
       "├─RoIHeads: 1-4                                    --                        [0, 4]                    --\n",
       "│    └─MultiScaleRoIAlign: 2-5                     --                        [10000, 256, 7, 7]        --\n",
       "│    └─FastRCNNConvFCHead: 2-6                     --                        [10000, 1024]             --\n",
       "│    │    └─Conv2dNormActivation: 3-33             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-34             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-35             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-36             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Flatten: 3-37                          --                        [10000, 12544]            --\n",
       "│    │    └─Linear: 3-38                           --                        [10000, 1024]             12,846,080\n",
       "│    │    └─ReLU: 3-39                             --                        [10000, 1024]             --\n",
       "│    └─FastRCNNPredictor: 2-7                      --                        [10000, 2]                --\n",
       "│    │    └─Linear: 3-40                           --                        [10000, 2]                2,050\n",
       "│    │    └─Linear: 3-41                           --                        [10000, 8]                8,200\n",
       "=============================================================================================================================\n",
       "Total params: 43,256,153\n",
       "Trainable params: 43,030,809\n",
       "Non-trainable params: 225,344\n",
       "Total mult-adds (Units.TERABYTES): 3.34\n",
       "=============================================================================================================================\n",
       "Input size (MB): 36.86\n",
       "Forward/backward pass size (MB): 47931.95\n",
       "Params size (MB): 173.02\n",
       "Estimated Total Size (MB): 48141.84\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model info of model with new head\n",
    "torchinfo.summary(oModel, (10, 3, 640, 480), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of images\n",
    "train_images_files = only_jpg_files(os.listdir(TRAIN_IMAGES_FOLDER))\n",
    "test_images_files = only_jpg_files(os.listdir(TEST_IMAGES_FOLDER))\n",
    "val_images_files = only_jpg_files(os.listdir(VAL_IMAGES_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dataset\n",
    "class ForeheadbboxDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images_list, data_typ, key_folders, transform=None):\n",
    "        self.images_list = images_list\n",
    "        self.data_typ = data_typ\n",
    "        self.transform = transform\n",
    "        self.key_folders=key_folders\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img=self.images_list[idx]\n",
    "        file_no_ext = Path(img).stem\n",
    "\n",
    "        #bounding box\n",
    "        bbox_folder = self.key_folders[self.data_typ]['labels']\n",
    "        bbox = get_bbox(file_no_ext, bbox_folder)\n",
    "        bbox=torch.tensor(bbox, dtype=torch.float32, device='cuda')\n",
    "\n",
    "        #labels\n",
    "        label=get_label(file_no_ext, bbox_folder)\n",
    "        labels=torch.tensor(np.array([label]), dtype=torch.int64, device='cuda')\n",
    "\n",
    "        #image\n",
    "        image_folder = self.key_folders[self.data_typ]['images']\n",
    "        full_image_filename = os.path.join(image_folder, img)\n",
    "        image=torchvision.io.read_image(full_image_filename).float() / 255.0\n",
    "        \n",
    "        #masks\n",
    "        masks_folder = self.key_folders[self.data_typ]['masks']\n",
    "        full_masks_filename = os.path.join(masks_folder, img)\n",
    "        mask=torchvision.io.read_image(full_masks_filename)\n",
    "        \n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "        # Ensure bbox is of size (1, 4)\n",
    "        if bbox.ndim == 1:\n",
    "            bbox = bbox.unsqueeze(0)\n",
    "\n",
    "        image_id = torch.tensor(np.array([idx]), dtype=torch.int64, device='cuda')\n",
    "        area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:,0])\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64, device='cuda')\n",
    "\n",
    "        # Wrap sample and targets into tensors:\n",
    "        target = {}\n",
    "        target[\"boxes\"]=bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"masks\"] = masks\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional transforms\n",
    "# currently not used and everything here already occurs in the dataset\n",
    "class ToTensor_imri(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, vy = sample['image'], sample['target'] \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        # image = np.transpose(image, (2, 0, 1))/255\n",
    "        return {'image': torch.Tensor(image),\n",
    "                'target': torch.Tensor(vy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating datasets\n",
    "dsTrain = ForeheadbboxDataset(train_images_files, 'train', key_to_image_folder, transform=None)#TorchVisionTrns.Compose([ToTensor_imri()]))\n",
    "dsVal   = ForeheadbboxDataset(val_images_files, 'val', key_to_image_folder, transform=None) #TorchVisionTrns.Compose([ToTensor_imri()]))\n",
    "dstest   = ForeheadbboxDataset(test_images_files, 'test', key_to_image_folder, transform=None) #TorchVisionTrns.Compose([ToTensor_imri()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders\n",
    "\n",
    "dlTrain, dlVal, dltest=build_dataset(batchSize,dsTrain,dsVal,dstest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "# gpu not working good on my laptop, should be changed\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n",
    "# runDevice = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m      4\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(runDevice) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43moModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Returns losses and detections\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:379\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 379\u001b[0m labels, matched_gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_targets_to_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[0;32m    381\u001b[0m loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[0;32m    382\u001b[0m     objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[0;32m    383\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:208\u001b[0m, in \u001b[0;36mRegionProposalNetwork.assign_targets_to_anchors\u001b[1;34m(self, anchors, targets)\u001b[0m\n\u001b[0;32m    206\u001b[0m     labels_per_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((anchors_per_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     match_quality_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors_per_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# get the targets corresponding GT for each proposal\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# NB: need to clamp the indices because we can have a single\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# GT in the image, and matched_idxs can be -2, which goes\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# out of bounds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\ops\\boxes.py:287\u001b[0m, in \u001b[0;36mbox_iou\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    286\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[1;32m--> 287\u001b[0m inter, union \u001b[38;5;241m=\u001b[39m \u001b[43m_box_inter_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m iou \u001b[38;5;241m=\u001b[39m inter \u001b[38;5;241m/\u001b[39m union\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\ops\\boxes.py:260\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[1;34m(boxes1, boxes2)\u001b[0m\n\u001b[0;32m    257\u001b[0m area1 \u001b[38;5;241m=\u001b[39m box_area(boxes1)\n\u001b[0;32m    258\u001b[0m area2 \u001b[38;5;241m=\u001b[39m box_area(boxes2)\n\u001b[1;32m--> 260\u001b[0m lt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m rb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m:], boxes2[:, \u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m    263\u001b[0m wh \u001b[38;5;241m=\u001b[39m _upcast(rb \u001b[38;5;241m-\u001b[39m lt)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "#testing if working\n",
    "images, targets = next(iter(dlTrain))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v.to(runDevice) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "output = oModel(images, targets)  # Returns losses and detections\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing all weights except the new head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting all parameters for length\n",
    "params = [p for p in oModel.parameters() if p.requires_grad]\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing all parameters\n",
    "for param in oModel.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreezing parameters for the new head\n",
    "for param in oModel.roi_heads.box_predictor.parameters():\n",
    "    param.requires_grad = True\n",
    "if NNType=='Mask RCNN':\n",
    "    for param in oModel.roi_heads.mask_predictor.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters length\n",
    "params2 = [p for p in oModel.parameters() if p.requires_grad]\n",
    "len(params2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {'method': 'random'}\n",
    "\n",
    "metric = {'name': 'loss','goal': 'minimize'}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {'optimizer': {'values': ['adam', 'sgd']},\n",
    "    'fc_layer_size': {'values': [128, 256, 512]},##\n",
    "    'dropout': {'values': [0.3, 0.4, 0.5]},}##\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({'epochs': {'value': 1}})\n",
    "\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {'distribution': 'uniform','min': 0.0001,'max': 0.1},# a flat distribution between 0.0001 and 0.1\n",
    "    'batch_size': {'values': [2,4,8,16]}})# integers between 2 and 16\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        dlTrain, dlVal, dltest = build_dataset(config.batch_size,dsTrain,dsVal,dstest)\n",
    "        oModel=build_network(numCls,NNType)\n",
    "\n",
    "        #freezing layers\n",
    "        for param in oModel.roi_heads.box_predictor.parameters():\n",
    "            param.requires_grad = True\n",
    "        if NNType=='Mask RCNN':\n",
    "            for param in oModel.roi_heads.mask_predictor.parameters():\n",
    "                param.requires_grad = True\n",
    "        oModel.to(runDevice)\n",
    "        optimizer = build_optimizer(oModel, config.optimizer, config.learning_rate)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "        for epoch in range(config.epochs):\n",
    "            # train for one epoch, printing every 10 iterations\n",
    "            logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
    "            # update the learning rate\n",
    "            lr_scheduler.step()\n",
    "            # evaluate on the test dataset\n",
    "            logger_eval=evaluate(oModel, dlVal, device=runDevice)#need to add meters from logger_eval to wandb logger, didn't get this far on CPU\n",
    "            wandb.log({\"loss\": logger.meters['loss'].avg, \"loss_classifier\":logger.meters['loss_classifier'].avg,\n",
    "                       \"loss_box_reg\":logger.meters['loss_box_reg'].avg, \"loss_objectness\":logger.meters['loss_objectness'].avg,\n",
    "                       \"loss_rpn_box_reg\":logger.meters['loss_rpn_box_reg'].avg,\"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000010  loss: 1.9234 (1.9234)  loss_classifier: 0.6832 (0.6832)  loss_box_reg: 0.0012 (0.0012)  loss_objectness: 0.3097 (0.3097)  loss_rpn_box_reg: 0.9293 (0.9293)\n",
      "Epoch: [0]  [   0/4550]  eta: 13:31:13  lr: 0.000010  loss: 1.9234 (1.9234)  loss_classifier: 0.6832 (0.6832)  loss_box_reg: 0.0012 (0.0012)  loss_objectness: 0.3097 (0.3097)  loss_rpn_box_reg: 0.9293 (0.9293)  time: 10.6974  data: 0.2992  max mem: 0\n",
      "lr: 0.000015  loss: 1.9234 (2.0006)  loss_classifier: 0.6797 (0.6814)  loss_box_reg: 0.0012 (0.0015)  loss_objectness: 0.3097 (0.3480)  loss_rpn_box_reg: 0.9293 (0.9697)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     a\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunDevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\Documents\\Imri\\ai\\project 2\\engine.py:31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     29\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 31\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m    104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m--> 105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:762\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    759\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    761\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_roi_pool(features, proposals, image_shapes)\n\u001b[1;32m--> 762\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m class_logits, box_regression \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_predictor(box_features)\n\u001b[0;32m    765\u001b[0m result: List[Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # training block, nothing is truely defined here with actual thought,everything is copied from tutorial,\n",
    "# # from here wanDB and maybe gridsearch/optuna should be defined and hyper parameters searched, \n",
    "# # maybe different optimizer and learning rate schedular, all weight are unfrozen\n",
    "\n",
    "# # move model to the right device\n",
    "# oModel.to(runDevice)\n",
    "\n",
    "# # construct an optimizer\n",
    "# params = [p for p in oModel.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     params,\n",
    "#     lr=0.005,\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0.0005\n",
    "# )\n",
    "\n",
    "# # and a learning rate scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=3,\n",
    "#     gamma=0.1\n",
    "# )\n",
    "\n",
    "# # let's train it just for 2 epochs\n",
    "# num_epochs = 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # train for one epoch, printing every 10 iterations\n",
    "#     a=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "#     # evaluate on the test dataset\n",
    "#     evaluate(oModel, dlVal, device=runDevice)\n",
    "\n",
    "# print(\"That's it!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
