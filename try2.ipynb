{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "# Miscellaneous\n",
    "import copy\n",
    "from enum import auto, Enum, unique\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact\n",
    "\n",
    "import wandb\n",
    "import utils_imri\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FOLDER = os.path.join('data', 'Forehead')\n",
    "TRAIN_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'train', 'images')\n",
    "TRAIN_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'train', 'labels')\n",
    "TRAIN_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'train', 'masks')\n",
    "VAL_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'val', 'images')\n",
    "VAL_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'val', 'labels')\n",
    "VAL_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'val', 'masks')\n",
    "TEST_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'test', 'images')\n",
    "TEST_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'test', 'labels')\n",
    "TEST_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'test', 'masks')\n",
    "key_to_image_folder = {\n",
    "    'train': {'images': TRAIN_IMAGES_FOLDER, 'labels': TRAIN_LABELS_FOLDER,'masks': TRAIN_MASKS_FOLDER},\n",
    "    'val': {'images': VAL_IMAGES_FOLDER, 'labels': VAL_LABELS_FOLDER,'masks': VAL_MASKS_FOLDER},\n",
    "    'test': {'images': TEST_IMAGES_FOLDER, 'labels': TEST_LABELS_FOLDER,'masks': TEST_MASKS_FOLDER}, \n",
    "}\n",
    "\n",
    "\n",
    "T_IMG_SIZE = (480, 640, 3)\n",
    "\n",
    "TENSOR_BOARD_BASE   = 'TB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "# numSamplesTrain = 30_000\n",
    "# numSamplesVal   = 10_000\n",
    "# boxFormat       = BBoxFormat.YOLO\n",
    "numCls          = 2 #<! Number of classes\n",
    "# maxObj          = 3\n",
    "\n",
    "# Model\n",
    "# gridSize = 5 #<! The gris is (gridSize x gridSize) \n",
    "\n",
    "# Training\n",
    "batchSize   = 4\n",
    "numWorkers  = 2 #<! Number of workers\n",
    "numEpochs   = 2\n",
    "λ = 20.0 #<! Localization Loss\n",
    "ϵ = 0.1 #<! Label Smoothing\n",
    "\n",
    "# Visualization\n",
    "# numImg = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary functions\n",
    "\n",
    "def only_jpg_files(files: List) -> List[str]:\n",
    "    return [item for item in files if item.endswith('jpg')]\n",
    "\n",
    "def get_bbox(filename: str, dirname: str) -> List[float]:\n",
    "    json_file = os.path.join(dirname, f'{filename}.json')\n",
    "    with open(json_file, 'r') as f:\n",
    "        bbox = json.load(f)['bbox']\n",
    "    return bbox\n",
    "\n",
    "def get_label(filename: str, dirname: str) -> List[int]:\n",
    "    json_file = os.path.join(dirname, f'{filename}.json')\n",
    "    with open(json_file, 'r') as f:\n",
    "        label = json.load(f)['class']\n",
    "    return label\n",
    "\n",
    "def build_dataset(batchsz,dsTrain,dsVal,dstest):\n",
    "\n",
    "    # ADD NUM_WORKERS ETC' when moving to gpu\n",
    "    train = DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchsz, collate_fn=utils_imri.collate_fn, num_workers = numWorkers, drop_last = True, persistent_workers = True)\n",
    "    val   = DataLoader(dsVal, shuffle = False, batch_size = 2 * batchsz, collate_fn=utils_imri.collate_fn, num_workers = numWorkers, persistent_workers = True)\n",
    "    test   = DataLoader(dstest, shuffle = False, batch_size = 2 * batchsz, collate_fn=utils_imri.collate_fn, num_workers = numWorkers, persistent_workers = True)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def build_network(num_classes, typ):\n",
    "    if typ=='Faster RCNN':\n",
    "        weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "        oModel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "        # get number of input features for the classifier\n",
    "        in_features = oModel.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        oModel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    elif typ=='Mask RCNN':\n",
    "        weights = MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "        oModel = maskrcnn_resnet50_fpn(weights=weights)\n",
    "        # get number of input features for the classifier\n",
    "        in_features = oModel.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        oModel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        # now get the number of input features for the mask classifier\n",
    "        in_features_mask = oModel.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        # and replace the mask predictor with a new one\n",
    "        oModel.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer,num_classes)\n",
    "\n",
    "    return oModel\n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9,weight_decay=0.0005)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate,betas = (0.9, 0.99), weight_decay = 2e-4)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "using Faster R-CNN with a Resnet50 Back bone with weights from a model trained on COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "# This can be changes to different weights/Model\n",
    "# suitable models can be found here https://pytorch.org/vision/stable/models.html#object-detection or\n",
    "# https://pytorch.org/vision/stable/models.html#instance-segmentation\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "oModel_org = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                             Kernel Shape              Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "FasterRCNN                                         --                        [0, 4]                    --\n",
       "├─GeneralizedRCNNTransform: 1-1                    --                        [10, 3, 1088, 800]        --\n",
       "├─BackboneWithFPN: 1-2                             --                        [10, 256, 17, 13]         --\n",
       "│    └─IntermediateLayerGetter: 2-1                --                        [10, 2048, 34, 25]        --\n",
       "│    │    └─Conv2d: 3-1                            [7, 7]                    [10, 64, 544, 400]        (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       --                        [10, 64, 544, 400]        (128)\n",
       "│    │    └─ReLU: 3-3                              --                        [10, 64, 544, 400]        --\n",
       "│    │    └─MaxPool2d: 3-4                         3                         [10, 64, 272, 200]        --\n",
       "│    │    └─Sequential: 3-5                        --                        [10, 256, 272, 200]       (215,808)\n",
       "│    │    └─Sequential: 3-6                        --                        [10, 512, 136, 100]       1,219,584\n",
       "│    │    └─Sequential: 3-7                        --                        [10, 1024, 68, 50]        7,098,368\n",
       "│    │    └─Sequential: 3-8                        --                        [10, 2048, 34, 25]        14,964,736\n",
       "│    └─FeaturePyramidNetwork: 2-2                  --                        [10, 256, 17, 13]         --\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-17                 --                        [10, 256, 272, 200]       --\n",
       "├─RegionProposalNetwork: 1-3                       --                        [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                --                        [10, 3, 272, 200]         --\n",
       "│    │    └─Sequential: 3-18                       --                        [10, 256, 272, 200]       1,180,160\n",
       "│    │    └─Conv2d: 3-19                           [1, 1]                    [10, 3, 272, 200]         771\n",
       "│    │    └─Conv2d: 3-20                           [1, 1]                    [10, 12, 272, 200]        3,084\n",
       "│    │    └─Sequential: 3-21                       --                        [10, 256, 136, 100]       (recursive)\n",
       "│    │    └─Conv2d: 3-22                           [1, 1]                    [10, 3, 136, 100]         (recursive)\n",
       "│    │    └─Conv2d: 3-23                           [1, 1]                    [10, 12, 136, 100]        (recursive)\n",
       "│    │    └─Sequential: 3-24                       --                        [10, 256, 68, 50]         (recursive)\n",
       "│    │    └─Conv2d: 3-25                           [1, 1]                    [10, 3, 68, 50]           (recursive)\n",
       "│    │    └─Conv2d: 3-26                           [1, 1]                    [10, 12, 68, 50]          (recursive)\n",
       "│    │    └─Sequential: 3-27                       --                        [10, 256, 34, 25]         (recursive)\n",
       "│    │    └─Conv2d: 3-28                           [1, 1]                    [10, 3, 34, 25]           (recursive)\n",
       "│    │    └─Conv2d: 3-29                           [1, 1]                    [10, 12, 34, 25]          (recursive)\n",
       "│    │    └─Sequential: 3-30                       --                        [10, 256, 17, 13]         (recursive)\n",
       "│    │    └─Conv2d: 3-31                           [1, 1]                    [10, 3, 17, 13]           (recursive)\n",
       "│    │    └─Conv2d: 3-32                           [1, 1]                    [10, 12, 17, 13]          (recursive)\n",
       "│    └─AnchorGenerator: 2-4                        --                        [217413, 4]               --\n",
       "├─RoIHeads: 1-4                                    --                        [0, 4]                    --\n",
       "│    └─MultiScaleRoIAlign: 2-5                     --                        [10000, 256, 7, 7]        --\n",
       "│    └─FastRCNNConvFCHead: 2-6                     --                        [10000, 1024]             --\n",
       "│    │    └─Conv2dNormActivation: 3-33             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-34             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-35             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-36             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Flatten: 3-37                          --                        [10000, 12544]            --\n",
       "│    │    └─Linear: 3-38                           --                        [10000, 1024]             12,846,080\n",
       "│    │    └─ReLU: 3-39                             --                        [10000, 1024]             --\n",
       "│    └─FastRCNNPredictor: 2-7                      --                        [10000, 91]               --\n",
       "│    │    └─Linear: 3-40                           --                        [10000, 91]               93,275\n",
       "│    │    └─Linear: 3-41                           --                        [10000, 364]              373,100\n",
       "=============================================================================================================================\n",
       "Total params: 43,712,278\n",
       "Trainable params: 43,486,934\n",
       "Non-trainable params: 225,344\n",
       "Total mult-adds (T): 3.35\n",
       "=============================================================================================================================\n",
       "Input size (MB): 36.86\n",
       "Forward/backward pass size (MB): 47967.55\n",
       "Params size (MB): 174.85\n",
       "Estimated Total Size (MB): 48179.27\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model as is \n",
    "\n",
    "torchinfo.summary(oModel_org, (10, 3, 640, 480), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "# This only changes the head\n",
    "\n",
    "# num_classes = 2  # 1 class (person) + background\n",
    "# build a network\n",
    "NNType='Faster RCNN'\n",
    "oModel=build_network(numCls,NNType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                             Kernel Shape              Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "FasterRCNN                                         --                        [0, 4]                    --\n",
       "├─GeneralizedRCNNTransform: 1-1                    --                        [10, 3, 1088, 800]        --\n",
       "├─BackboneWithFPN: 1-2                             --                        [10, 256, 17, 13]         --\n",
       "│    └─IntermediateLayerGetter: 2-1                --                        [10, 2048, 34, 25]        --\n",
       "│    │    └─Conv2d: 3-1                            [7, 7]                    [10, 64, 544, 400]        (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       --                        [10, 64, 544, 400]        (128)\n",
       "│    │    └─ReLU: 3-3                              --                        [10, 64, 544, 400]        --\n",
       "│    │    └─MaxPool2d: 3-4                         3                         [10, 64, 272, 200]        --\n",
       "│    │    └─Sequential: 3-5                        --                        [10, 256, 272, 200]       (215,808)\n",
       "│    │    └─Sequential: 3-6                        --                        [10, 512, 136, 100]       1,219,584\n",
       "│    │    └─Sequential: 3-7                        --                        [10, 1024, 68, 50]        7,098,368\n",
       "│    │    └─Sequential: 3-8                        --                        [10, 2048, 34, 25]        14,964,736\n",
       "│    └─FeaturePyramidNetwork: 2-2                  --                        [10, 256, 17, 13]         --\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-17                 --                        [10, 256, 272, 200]       --\n",
       "├─RegionProposalNetwork: 1-3                       --                        [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                --                        [10, 3, 272, 200]         --\n",
       "│    │    └─Sequential: 3-18                       --                        [10, 256, 272, 200]       1,180,160\n",
       "│    │    └─Conv2d: 3-19                           [1, 1]                    [10, 3, 272, 200]         771\n",
       "│    │    └─Conv2d: 3-20                           [1, 1]                    [10, 12, 272, 200]        3,084\n",
       "│    │    └─Sequential: 3-21                       --                        [10, 256, 136, 100]       (recursive)\n",
       "│    │    └─Conv2d: 3-22                           [1, 1]                    [10, 3, 136, 100]         (recursive)\n",
       "│    │    └─Conv2d: 3-23                           [1, 1]                    [10, 12, 136, 100]        (recursive)\n",
       "│    │    └─Sequential: 3-24                       --                        [10, 256, 68, 50]         (recursive)\n",
       "│    │    └─Conv2d: 3-25                           [1, 1]                    [10, 3, 68, 50]           (recursive)\n",
       "│    │    └─Conv2d: 3-26                           [1, 1]                    [10, 12, 68, 50]          (recursive)\n",
       "│    │    └─Sequential: 3-27                       --                        [10, 256, 34, 25]         (recursive)\n",
       "│    │    └─Conv2d: 3-28                           [1, 1]                    [10, 3, 34, 25]           (recursive)\n",
       "│    │    └─Conv2d: 3-29                           [1, 1]                    [10, 12, 34, 25]          (recursive)\n",
       "│    │    └─Sequential: 3-30                       --                        [10, 256, 17, 13]         (recursive)\n",
       "│    │    └─Conv2d: 3-31                           [1, 1]                    [10, 3, 17, 13]           (recursive)\n",
       "│    │    └─Conv2d: 3-32                           [1, 1]                    [10, 12, 17, 13]          (recursive)\n",
       "│    └─AnchorGenerator: 2-4                        --                        [217413, 4]               --\n",
       "├─RoIHeads: 1-4                                    --                        [0, 4]                    --\n",
       "│    └─MultiScaleRoIAlign: 2-5                     --                        [10000, 256, 7, 7]        --\n",
       "│    └─FastRCNNConvFCHead: 2-6                     --                        [10000, 1024]             --\n",
       "│    │    └─Conv2dNormActivation: 3-33             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-34             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-35             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-36             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Flatten: 3-37                          --                        [10000, 12544]            --\n",
       "│    │    └─Linear: 3-38                           --                        [10000, 1024]             12,846,080\n",
       "│    │    └─ReLU: 3-39                             --                        [10000, 1024]             --\n",
       "│    └─FastRCNNPredictor: 2-7                      --                        [10000, 2]                --\n",
       "│    │    └─Linear: 3-40                           --                        [10000, 2]                2,050\n",
       "│    │    └─Linear: 3-41                           --                        [10000, 8]                8,200\n",
       "=============================================================================================================================\n",
       "Total params: 43,256,153\n",
       "Trainable params: 43,030,809\n",
       "Non-trainable params: 225,344\n",
       "Total mult-adds (T): 3.34\n",
       "=============================================================================================================================\n",
       "Input size (MB): 36.86\n",
       "Forward/backward pass size (MB): 47931.95\n",
       "Params size (MB): 173.02\n",
       "Estimated Total Size (MB): 48141.84\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model info of model with new head\n",
    "torchinfo.summary(oModel, (10, 3, 640, 480), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of images\n",
    "train_images_files = only_jpg_files(os.listdir(TRAIN_IMAGES_FOLDER))\n",
    "test_images_files = only_jpg_files(os.listdir(TEST_IMAGES_FOLDER))\n",
    "val_images_files = only_jpg_files(os.listdir(VAL_IMAGES_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dataset\n",
    "class ForeheadbboxDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images_list, data_typ, key_folders, transform=None):\n",
    "        self.images_list = images_list\n",
    "        self.data_typ = data_typ\n",
    "        self.transform = transform\n",
    "        self.key_folders=key_folders\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img=self.images_list[idx]\n",
    "        file_no_ext = Path(img).stem\n",
    "\n",
    "        #bounding box\n",
    "        bbox_folder = self.key_folders[self.data_typ]['labels']\n",
    "        bbox = get_bbox(file_no_ext, bbox_folder)\n",
    "        bbox=torch.tensor(bbox, dtype=torch.float32, device='cuda')\n",
    "\n",
    "        #labels\n",
    "        label=get_label(file_no_ext, bbox_folder)\n",
    "        labels=torch.tensor(np.array([label]), dtype=torch.int64, device='cuda')\n",
    "\n",
    "        #image\n",
    "        image_folder = self.key_folders[self.data_typ]['images']\n",
    "        full_image_filename = os.path.join(image_folder, img)\n",
    "        image=torchvision.io.read_image(full_image_filename).float() / 255.0\n",
    "        \n",
    "        #masks\n",
    "        masks_folder = self.key_folders[self.data_typ]['masks']\n",
    "        full_masks_filename = os.path.join(masks_folder, img)\n",
    "        mask=torchvision.io.read_image(full_masks_filename)\n",
    "        \n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "        # Ensure bbox is of size (1, 4)\n",
    "        if bbox.ndim == 1:\n",
    "            bbox = bbox.unsqueeze(0)\n",
    "\n",
    "        image_id = torch.tensor(np.array([idx]), dtype=torch.int64, device='cuda')\n",
    "        area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:,0])\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64, device='cuda')\n",
    "\n",
    "        # Wrap sample and targets into tensors:\n",
    "        target = {}\n",
    "        target[\"boxes\"]=bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"masks\"] = masks\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional transforms\n",
    "# currently not used and everything here already occurs in the dataset\n",
    "class ToTensor_imri(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, vy = sample['image'], sample['target'] \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        # image = np.transpose(image, (2, 0, 1))/255\n",
    "        return {'image': torch.Tensor(image),\n",
    "                'target': torch.Tensor(vy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating datasets\n",
    "dsTrain = ForeheadbboxDataset(train_images_files, 'train', key_to_image_folder, transform=None)#TorchVisionTrns.Compose([ToTensor_imri()]))\n",
    "dsVal   = ForeheadbboxDataset(val_images_files, 'val', key_to_image_folder, transform=None) #TorchVisionTrns.Compose([ToTensor_imri()]))\n",
    "dstest   = ForeheadbboxDataset(test_images_files, 'test', key_to_image_folder, transform=None) #TorchVisionTrns.Compose([ToTensor_imri()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders\n",
    "\n",
    "dlTrain, dlVal, dltest=build_dataset(batchSize,dsTrain,dsVal,dstest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "# gpu not working good on my laptop, should be changed\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n",
    "# runDevice = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_24046/1515537592.py\", line 21, in __getitem__\n    bbox=torch.tensor(bbox, dtype=torch.float32, device='cuda')\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#testing if working\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdlTrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m      4\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(runDevice) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_24046/1515537592.py\", line 21, in __getitem__\n    bbox=torch.tensor(bbox, dtype=torch.float32, device='cuda')\n  File \"/home/ubuntu/.pyenv/versions/3.10.14/envs/forehead_venv/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "#testing if working\n",
    "images, targets = next(iter(dlTrain))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v.to(runDevice) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "output = oModel(images, targets)  # Returns losses and detections\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing all weights except the new head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting all parameters for length\n",
    "params = [p for p in oModel.parameters() if p.requires_grad]\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing all parameters\n",
    "for param in oModel.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreezing parameters for the new head\n",
    "for param in oModel.roi_heads.box_predictor.parameters():\n",
    "    param.requires_grad = True\n",
    "if NNType=='Mask RCNN':\n",
    "    for param in oModel.roi_heads.mask_predictor.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters length\n",
    "params2 = [p for p in oModel.parameters() if p.requires_grad]\n",
    "len(params2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {'method': 'random'}\n",
    "\n",
    "metric = {'name': 'loss','goal': 'minimize'}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {'optimizer': {'values': ['adam', 'sgd']},\n",
    "    'fc_layer_size': {'values': [128, 256, 512]},##\n",
    "    'dropout': {'values': [0.3, 0.4, 0.5]},}##\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({'epochs': {'value': 1}})\n",
    "\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {'distribution': 'uniform','min': 0.0001,'max': 0.1},# a flat distribution between 0.0001 and 0.1\n",
    "    'batch_size': {'values': [2,4,8,16]}})# integers between 2 and 16\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        dlTrain, dlVal, dltest = build_dataset(config.batch_size,dsTrain,dsVal,dstest)\n",
    "        oModel=build_network(numCls,NNType)\n",
    "\n",
    "        #freezing layers\n",
    "        for param in oModel.roi_heads.box_predictor.parameters():\n",
    "            param.requires_grad = True\n",
    "        if NNType=='Mask RCNN':\n",
    "            for param in oModel.roi_heads.mask_predictor.parameters():\n",
    "                param.requires_grad = True\n",
    "        oModel.to(runDevice)\n",
    "        optimizer = build_optimizer(oModel, config.optimizer, config.learning_rate)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "        for epoch in range(config.epochs):\n",
    "            # train for one epoch, printing every 10 iterations\n",
    "            logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
    "            # update the learning rate\n",
    "            lr_scheduler.step()\n",
    "            # evaluate on the test dataset\n",
    "            logger_eval=evaluate(oModel, dlVal, device=runDevice)#need to add meters from logger_eval to wandb logger, didn't get this far on CPU\n",
    "            wandb.log({\"loss\": logger.meters['loss'].avg, \"loss_classifier\":logger.meters['loss_classifier'].avg,\n",
    "                       \"loss_box_reg\":logger.meters['loss_box_reg'].avg, \"loss_objectness\":logger.meters['loss_objectness'].avg,\n",
    "                       \"loss_rpn_box_reg\":logger.meters['loss_rpn_box_reg'].avg,\"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000010  loss: 1.9234 (1.9234)  loss_classifier: 0.6832 (0.6832)  loss_box_reg: 0.0012 (0.0012)  loss_objectness: 0.3097 (0.3097)  loss_rpn_box_reg: 0.9293 (0.9293)\n",
      "Epoch: [0]  [   0/4550]  eta: 13:31:13  lr: 0.000010  loss: 1.9234 (1.9234)  loss_classifier: 0.6832 (0.6832)  loss_box_reg: 0.0012 (0.0012)  loss_objectness: 0.3097 (0.3097)  loss_rpn_box_reg: 0.9293 (0.9293)  time: 10.6974  data: 0.2992  max mem: 0\n",
      "lr: 0.000015  loss: 1.9234 (2.0006)  loss_classifier: 0.6797 (0.6814)  loss_box_reg: 0.0012 (0.0015)  loss_objectness: 0.3097 (0.3480)  loss_rpn_box_reg: 0.9293 (0.9697)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     a\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunDevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\Documents\\Imri\\ai\\project 2\\engine.py:31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     29\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 31\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m    104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m--> 105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:762\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    759\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    761\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_roi_pool(features, proposals, image_shapes)\n\u001b[1;32m--> 762\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m class_logits, box_regression \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_predictor(box_features)\n\u001b[0;32m    765\u001b[0m result: List[Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # training block, nothing is truely defined here with actual thought,everything is copied from tutorial,\n",
    "# # from here wanDB and maybe gridsearch/optuna should be defined and hyper parameters searched, \n",
    "# # maybe different optimizer and learning rate schedular, all weight are unfrozen\n",
    "\n",
    "# # move model to the right device\n",
    "# oModel.to(runDevice)\n",
    "\n",
    "# # construct an optimizer\n",
    "# params = [p for p in oModel.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     params,\n",
    "#     lr=0.005,\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0.0005\n",
    "# )\n",
    "\n",
    "# # and a learning rate scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=3,\n",
    "#     gamma=0.1\n",
    "# )\n",
    "\n",
    "# # let's train it just for 2 epochs\n",
    "# num_epochs = 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # train for one epoch, printing every 10 iterations\n",
    "#     a=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "#     # evaluate on the test dataset\n",
    "#     evaluate(oModel, dlVal, device=runDevice)\n",
    "\n",
    "# print(\"That's it!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
