{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchinfo\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as TorchVisionTrns\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "\n",
    "\n",
    "\n",
    "# Miscellaneous\n",
    "import copy\n",
    "from enum import auto, Enum, unique\n",
    "import math\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "\n",
    "# Typing\n",
    "from typing import Callable, Dict, Generator, List, Optional, Self, Set, Tuple, Union\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Image\n",
    "from IPython.display import display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout, SelectionSlider\n",
    "from ipywidgets import interact\n",
    "\n",
    "import wandb\n",
    "import utils_imri as utils\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# Matplotlib default color palette\n",
    "lMatPltLibclr = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "# Improve performance by benchmarking\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Reproducibility (Per PyTorch Version on the same device)\n",
    "# torch.manual_seed(seedNum)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark     = False #<! Makes things slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF    = (8, 8)\n",
    "ELM_SIZE_DEF    = 50\n",
    "CLASS_COLOR     = ('b', 'r')\n",
    "EDGE_COLOR      = 'k'\n",
    "MARKER_SIZE_DEF = 10\n",
    "LINE_WIDTH_DEF  = 2\n",
    "\n",
    "DATA_FOLDER = os.path.join('data', 'Forehead')\n",
    "TRAIN_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'train', 'images')\n",
    "TRAIN_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'train', 'labels')\n",
    "TRAIN_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'train', 'masks')\n",
    "VAL_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'val', 'images')\n",
    "VAL_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'val', 'labels')\n",
    "VAL_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'val', 'masks')\n",
    "TEST_IMAGES_FOLDER = os.path.join(DATA_FOLDER, 'test', 'images')\n",
    "TEST_LABELS_FOLDER = os.path.join(DATA_FOLDER, 'test', 'labels')\n",
    "TEST_MASKS_FOLDER = os.path.join(DATA_FOLDER, 'test', 'masks')\n",
    "key_to_image_folder = {\n",
    "    'train': {'images': TRAIN_IMAGES_FOLDER, 'labels': TRAIN_LABELS_FOLDER,'masks': TRAIN_MASKS_FOLDER},\n",
    "    'val': {'images': VAL_IMAGES_FOLDER, 'labels': VAL_LABELS_FOLDER,'masks': VAL_MASKS_FOLDER},\n",
    "    'test': {'images': TEST_IMAGES_FOLDER, 'labels': TEST_LABELS_FOLDER,'masks': TEST_MASKS_FOLDER}, \n",
    "}\n",
    "\n",
    "\n",
    "T_IMG_SIZE = (480, 640, 3)\n",
    "\n",
    "TENSOR_BOARD_BASE   = 'TB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data\n",
    "# numSamplesTrain = 30_000\n",
    "# numSamplesVal   = 10_000\n",
    "# boxFormat       = BBoxFormat.YOLO\n",
    "numCls          = 2 #<! Number of classes\n",
    "# maxObj          = 3\n",
    "\n",
    "# Model\n",
    "# gridSize = 5 #<! The gris is (gridSize x gridSize) \n",
    "\n",
    "# Training\n",
    "batchSize   = 4\n",
    "numWorkers  = 2 #<! Number of workers\n",
    "numEpochs   = 2\n",
    "λ = 20.0 #<! Localization Loss\n",
    "ϵ = 0.1 #<! Label Smoothing\n",
    "\n",
    "# Visualization\n",
    "# numImg = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary functions\n",
    "\n",
    "def only_jpg_files(files: List) -> List[str]:\n",
    "    return [item for item in files if item.endswith('jpg')]\n",
    "\n",
    "def get_bbox(filename: str, dirname: str) -> List[float]:\n",
    "    json_file = os.path.join(dirname, f'{filename}.json')\n",
    "    with open(json_file, 'r') as f:\n",
    "        bbox = json.load(f)['bbox']\n",
    "    return bbox\n",
    "\n",
    "def get_label(filename: str, dirname: str) -> List[int]:\n",
    "    json_file = os.path.join(dirname, f'{filename}.json')\n",
    "    with open(json_file, 'r') as f:\n",
    "        label = json.load(f)['class']\n",
    "    return label\n",
    "\n",
    "def build_dataset(batchsz,dsTrain,dsVal,dstest):\n",
    "\n",
    "    # ADD NUM_WORKERS ETC' when moving to gpu\n",
    "    train = DataLoader(dsTrain, shuffle = True, batch_size = 1 * batchsz, collate_fn=utils.collate_fn)\n",
    "    val   = DataLoader(dsVal, shuffle = False, batch_size = 2 * batchsz, collate_fn=utils.collate_fn)\n",
    "    test   = DataLoader(dstest, shuffle = False, batch_size = 2 * batchsz, collate_fn=utils.collate_fn)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def build_network(num_classes, typ):\n",
    "    if typ=='Faster RCNN':\n",
    "        weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "        oModel = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "        # get number of input features for the classifier\n",
    "        in_features = oModel.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        oModel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    elif typ=='Mask RCNN':\n",
    "        weights = MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "        oModel = maskrcnn_resnet50_fpn_v2(weights=weights)\n",
    "        # get number of input features for the classifier\n",
    "        in_features = oModel.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        oModel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        # now get the number of input features for the mask classifier\n",
    "        in_features_mask = oModel.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        # and replace the mask predictor with a new one\n",
    "        oModel.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer,num_classes)\n",
    "\n",
    "    return oModel\n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9,weight_decay=0.0005)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate,betas = (0.9, 0.99), weight_decay = 2e-4)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of images\n",
    "train_images_files = only_jpg_files(os.listdir(TRAIN_IMAGES_FOLDER))\n",
    "test_images_files = only_jpg_files(os.listdir(TEST_IMAGES_FOLDER))\n",
    "val_images_files = only_jpg_files(os.listdir(VAL_IMAGES_FOLDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "using Faster R-CNN with a Resnet50 Back bone with weights from a model trained on COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "# This can be changes to different weights/Model\n",
    "# suitable models can be found here https://pytorch.org/vision/stable/models.html#object-detection or\n",
    "# https://pytorch.org/vision/stable/models.html#instance-segmentation\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "oModel_org = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                             Kernel Shape              Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "FasterRCNN                                         --                        [0, 4]                    --\n",
       "├─GeneralizedRCNNTransform: 1-1                    --                        [10, 3, 1088, 800]        --\n",
       "├─BackboneWithFPN: 1-2                             --                        [10, 256, 17, 13]         --\n",
       "│    └─IntermediateLayerGetter: 2-1                --                        [10, 2048, 34, 25]        --\n",
       "│    │    └─Conv2d: 3-1                            [7, 7]                    [10, 64, 544, 400]        (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       --                        [10, 64, 544, 400]        (128)\n",
       "│    │    └─ReLU: 3-3                              --                        [10, 64, 544, 400]        --\n",
       "│    │    └─MaxPool2d: 3-4                         3                         [10, 64, 272, 200]        --\n",
       "│    │    └─Sequential: 3-5                        --                        [10, 256, 272, 200]       (215,808)\n",
       "│    │    └─Sequential: 3-6                        --                        [10, 512, 136, 100]       1,219,584\n",
       "│    │    └─Sequential: 3-7                        --                        [10, 1024, 68, 50]        7,098,368\n",
       "│    │    └─Sequential: 3-8                        --                        [10, 2048, 34, 25]        14,964,736\n",
       "│    └─FeaturePyramidNetwork: 2-2                  --                        [10, 256, 17, 13]         --\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-17                 --                        [10, 256, 272, 200]       --\n",
       "├─RegionProposalNetwork: 1-3                       --                        [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                --                        [10, 3, 272, 200]         --\n",
       "│    │    └─Sequential: 3-18                       --                        [10, 256, 272, 200]       1,180,160\n",
       "│    │    └─Conv2d: 3-19                           [1, 1]                    [10, 3, 272, 200]         771\n",
       "│    │    └─Conv2d: 3-20                           [1, 1]                    [10, 12, 272, 200]        3,084\n",
       "│    │    └─Sequential: 3-21                       --                        [10, 256, 136, 100]       (recursive)\n",
       "│    │    └─Conv2d: 3-22                           [1, 1]                    [10, 3, 136, 100]         (recursive)\n",
       "│    │    └─Conv2d: 3-23                           [1, 1]                    [10, 12, 136, 100]        (recursive)\n",
       "│    │    └─Sequential: 3-24                       --                        [10, 256, 68, 50]         (recursive)\n",
       "│    │    └─Conv2d: 3-25                           [1, 1]                    [10, 3, 68, 50]           (recursive)\n",
       "│    │    └─Conv2d: 3-26                           [1, 1]                    [10, 12, 68, 50]          (recursive)\n",
       "│    │    └─Sequential: 3-27                       --                        [10, 256, 34, 25]         (recursive)\n",
       "│    │    └─Conv2d: 3-28                           [1, 1]                    [10, 3, 34, 25]           (recursive)\n",
       "│    │    └─Conv2d: 3-29                           [1, 1]                    [10, 12, 34, 25]          (recursive)\n",
       "│    │    └─Sequential: 3-30                       --                        [10, 256, 17, 13]         (recursive)\n",
       "│    │    └─Conv2d: 3-31                           [1, 1]                    [10, 3, 17, 13]           (recursive)\n",
       "│    │    └─Conv2d: 3-32                           [1, 1]                    [10, 12, 17, 13]          (recursive)\n",
       "│    └─AnchorGenerator: 2-4                        --                        [217413, 4]               --\n",
       "├─RoIHeads: 1-4                                    --                        [0, 4]                    --\n",
       "│    └─MultiScaleRoIAlign: 2-5                     --                        [10000, 256, 7, 7]        --\n",
       "│    └─FastRCNNConvFCHead: 2-6                     --                        [10000, 1024]             --\n",
       "│    │    └─Conv2dNormActivation: 3-33             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-34             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-35             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-36             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Flatten: 3-37                          --                        [10000, 12544]            --\n",
       "│    │    └─Linear: 3-38                           --                        [10000, 1024]             12,846,080\n",
       "│    │    └─ReLU: 3-39                             --                        [10000, 1024]             --\n",
       "│    └─FastRCNNPredictor: 2-7                      --                        [10000, 91]               --\n",
       "│    │    └─Linear: 3-40                           --                        [10000, 91]               93,275\n",
       "│    │    └─Linear: 3-41                           --                        [10000, 364]              373,100\n",
       "=============================================================================================================================\n",
       "Total params: 43,712,278\n",
       "Trainable params: 43,486,934\n",
       "Non-trainable params: 225,344\n",
       "Total mult-adds (Units.TERABYTES): 3.35\n",
       "=============================================================================================================================\n",
       "Input size (MB): 36.86\n",
       "Forward/backward pass size (MB): 47967.55\n",
       "Params size (MB): 174.85\n",
       "Estimated Total Size (MB): 48179.27\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model as is \n",
    "\n",
    "torchinfo.summary(oModel_org, (10, 3, 640, 480), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "# This only changes the head\n",
    "\n",
    "# num_classes = 2  # 1 class (person) + background\n",
    "# build a network\n",
    "# NNType='Faster RCNN'\n",
    "NNType='Mask RCNN'\n",
    "oModel=build_network(numCls,NNType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                             Kernel Shape              Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "MaskRCNN                                           --                        [100, 4]                  --\n",
       "├─GeneralizedRCNNTransform: 1-1                    --                        [10, 3, 1088, 800]        --\n",
       "├─BackboneWithFPN: 1-2                             --                        [10, 256, 17, 13]         --\n",
       "│    └─IntermediateLayerGetter: 2-1                --                        [10, 2048, 34, 25]        --\n",
       "│    │    └─Conv2d: 3-1                            [7, 7]                    [10, 64, 544, 400]        (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       --                        [10, 64, 544, 400]        (128)\n",
       "│    │    └─ReLU: 3-3                              --                        [10, 64, 544, 400]        --\n",
       "│    │    └─MaxPool2d: 3-4                         3                         [10, 64, 272, 200]        --\n",
       "│    │    └─Sequential: 3-5                        --                        [10, 256, 272, 200]       (215,808)\n",
       "│    │    └─Sequential: 3-6                        --                        [10, 512, 136, 100]       1,219,584\n",
       "│    │    └─Sequential: 3-7                        --                        [10, 1024, 68, 50]        7,098,368\n",
       "│    │    └─Sequential: 3-8                        --                        [10, 2048, 34, 25]        14,964,736\n",
       "│    └─FeaturePyramidNetwork: 2-2                  --                        [10, 256, 17, 13]         --\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        (recursive)\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        (recursive)\n",
       "│    │    └─LastLevelMaxPool: 3-17                 --                        [10, 256, 272, 200]       --\n",
       "├─RegionProposalNetwork: 1-3                       --                        [1000, 4]                 --\n",
       "│    └─RPNHead: 2-3                                --                        [10, 3, 272, 200]         --\n",
       "│    │    └─Sequential: 3-18                       --                        [10, 256, 272, 200]       1,180,160\n",
       "│    │    └─Conv2d: 3-19                           [1, 1]                    [10, 3, 272, 200]         771\n",
       "│    │    └─Conv2d: 3-20                           [1, 1]                    [10, 12, 272, 200]        3,084\n",
       "│    │    └─Sequential: 3-21                       --                        [10, 256, 136, 100]       (recursive)\n",
       "│    │    └─Conv2d: 3-22                           [1, 1]                    [10, 3, 136, 100]         (recursive)\n",
       "│    │    └─Conv2d: 3-23                           [1, 1]                    [10, 12, 136, 100]        (recursive)\n",
       "│    │    └─Sequential: 3-24                       --                        [10, 256, 68, 50]         (recursive)\n",
       "│    │    └─Conv2d: 3-25                           [1, 1]                    [10, 3, 68, 50]           (recursive)\n",
       "│    │    └─Conv2d: 3-26                           [1, 1]                    [10, 12, 68, 50]          (recursive)\n",
       "│    │    └─Sequential: 3-27                       --                        [10, 256, 34, 25]         (recursive)\n",
       "│    │    └─Conv2d: 3-28                           [1, 1]                    [10, 3, 34, 25]           (recursive)\n",
       "│    │    └─Conv2d: 3-29                           [1, 1]                    [10, 12, 34, 25]          (recursive)\n",
       "│    │    └─Sequential: 3-30                       --                        [10, 256, 17, 13]         (recursive)\n",
       "│    │    └─Conv2d: 3-31                           [1, 1]                    [10, 3, 17, 13]           (recursive)\n",
       "│    │    └─Conv2d: 3-32                           [1, 1]                    [10, 12, 17, 13]          (recursive)\n",
       "│    └─AnchorGenerator: 2-4                        --                        [217413, 4]               --\n",
       "├─RoIHeads: 1-4                                    --                        [100, 4]                  --\n",
       "│    └─MultiScaleRoIAlign: 2-5                     --                        [10000, 256, 7, 7]        --\n",
       "│    └─FastRCNNConvFCHead: 2-6                     --                        [10000, 1024]             --\n",
       "│    │    └─Conv2dNormActivation: 3-33             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-34             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-35             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Conv2dNormActivation: 3-36             --                        [10000, 256, 7, 7]        590,336\n",
       "│    │    └─Flatten: 3-37                          --                        [10000, 12544]            --\n",
       "│    │    └─Linear: 3-38                           --                        [10000, 1024]             12,846,080\n",
       "│    │    └─ReLU: 3-39                             --                        [10000, 1024]             --\n",
       "│    └─FastRCNNPredictor: 2-7                      --                        [10000, 2]                --\n",
       "│    │    └─Linear: 3-40                           --                        [10000, 2]                2,050\n",
       "│    │    └─Linear: 3-41                           --                        [10000, 8]                8,200\n",
       "│    └─MultiScaleRoIAlign: 2-8                     --                        [1000, 256, 14, 14]       --\n",
       "│    └─MaskRCNNHeads: 2-9                          --                        [1000, 256, 14, 14]       --\n",
       "│    │    └─Conv2dNormActivation: 3-42             --                        [1000, 256, 14, 14]       590,336\n",
       "│    │    └─Conv2dNormActivation: 3-43             --                        [1000, 256, 14, 14]       590,336\n",
       "│    │    └─Conv2dNormActivation: 3-44             --                        [1000, 256, 14, 14]       590,336\n",
       "│    │    └─Conv2dNormActivation: 3-45             --                        [1000, 256, 14, 14]       590,336\n",
       "│    └─MaskRCNNPredictor: 2-10                     --                        [1000, 2, 28, 28]         --\n",
       "│    │    └─ConvTranspose2d: 3-46                  [2, 2]                    [1000, 256, 28, 28]       262,400\n",
       "│    │    └─ReLU: 3-47                             --                        [1000, 256, 28, 28]       --\n",
       "│    │    └─Conv2d: 3-48                           [1, 1]                    [1000, 2, 28, 28]         514\n",
       "=============================================================================================================================\n",
       "Total params: 45,880,411\n",
       "Trainable params: 45,655,067\n",
       "Non-trainable params: 225,344\n",
       "Total mult-adds (Units.TERABYTES): 4.01\n",
       "=============================================================================================================================\n",
       "Input size (MB): 36.86\n",
       "Forward/backward pass size (MB): 52761.39\n",
       "Params size (MB): 183.52\n",
       "Estimated Total Size (MB): 52981.78\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model info of model with new head\n",
    "torchinfo.summary(oModel, (10, 3, 640, 480), col_names = ['kernel_size', 'output_size', 'num_params'], device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dataset\n",
    "class ForeheadbboxDataset_old(Dataset):\n",
    "\n",
    "    def __init__(self, images_list, data_typ, key_folders, transform=None):\n",
    "        self.images_list = images_list\n",
    "        self.data_typ = data_typ\n",
    "        self.transform = transform\n",
    "        self.key_folders=key_folders\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img=self.images_list[idx]\n",
    "        file_no_ext = Path(img).stem\n",
    "\n",
    "        #bounding box\n",
    "        bbox_folder = self.key_folders[self.data_typ]['labels']\n",
    "        bbox = get_bbox(file_no_ext, bbox_folder)\n",
    "        bbox=torch.tensor(bbox, dtype=torch.float32)\n",
    "        b=np.array([640,480,640,480])\n",
    "        bbox=torch.round(bbox*b.T).to(dtype=torch.float32)\n",
    "\n",
    "        #labels\n",
    "        label=get_label(file_no_ext, bbox_folder)\n",
    "        labels=torch.tensor(np.array([label]), dtype=torch.int64, device='cuda')\n",
    "\n",
    "        #image\n",
    "        image_folder = self.key_folders[self.data_typ]['images']\n",
    "        full_image_filename = os.path.join(image_folder, img)\n",
    "        image=torchvision.io.read_image(full_image_filename).float() / 255.0\n",
    "        \n",
    "        #masks\n",
    "        masks_folder = self.key_folders[self.data_typ]['masks']\n",
    "        full_masks_filename = os.path.join(masks_folder, img)\n",
    "        mask=torchvision.io.read_image(full_masks_filename)\n",
    "        \n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "        # Ensure bbox is of size (1, 4)\n",
    "        if bbox.ndim == 1:\n",
    "            bbox = bbox.unsqueeze(0)\n",
    "\n",
    "        image_id = torch.tensor(np.array([idx]), dtype=torch.int64)\n",
    "        area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:,0])\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        image= torchvision.tv_tensors.Image(image)\n",
    "\n",
    "        # Wrap sample and targets into tensors:\n",
    "        target = {}\n",
    "        target[\"boxes\"]=torchvision.tv_tensors.BoundingBoxes(bbox, format=\"XYXY\", canvas_size=TorchVisionTrns.functional.get_size(image))\n",
    "        target[\"labels\"] = labels\n",
    "        # target[\"image_id\"] = image_id\n",
    "        target[\"image_id\"] = idx\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"masks\"] = torchvision.tv_tensors.Mask(masks)\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForeheadbboxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = torchvision.tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torchvision.tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=TorchVisionTrns.functional.get_size(img))\n",
    "        target[\"masks\"] = torchvision.tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(TorchVisionTrns.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(TorchVisionTrns.ToPureTensor())\n",
    "    return TorchVisionTrns.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ForeheadbboxDataset('data/Forehead/train', get_transform(train=True))\n",
    "dataset_test = ForeheadbboxDataset('data/Forehead/train', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "dlTrain = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "dlval = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Device\n",
    "# gpu not working good on my laptop, should be changed\n",
    "runDevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #<! The 1st CUDA device\n",
    "# runDevice = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing if working\n",
    "# images, targets = next(iter(dlTrain))\n",
    "# images = list(image.to(runDevice) for image in images)\n",
    "# targets = [{k: v.to(runDevice) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "# oModel.to(runDevice)\n",
    "# output = oModel(images, targets)  # Returns losses and detections\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing all weights except the new head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting all parameters for length\n",
    "params = [p for p in oModel.parameters() if p.requires_grad]\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing all parameters\n",
    "for param in oModel.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreezing parameters for the new head\n",
    "for param in oModel.roi_heads.box_predictor.parameters():\n",
    "    param.requires_grad = True\n",
    "if NNType=='Mask RCNN':\n",
    "    for param in oModel.roi_heads.mask_predictor.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters length\n",
    "params2 = [p for p in oModel.parameters() if p.requires_grad]\n",
    "len(params2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimrif01\u001b[0m (\u001b[33mimri\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'batch_size': {'values': [16]},\n",
      "                'epochs': {'value': 4},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.1,\n",
      "                                  'min': 0.0001},\n",
      "                'optimizer': {'values': ['adam', 'sgd']}}}\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {'method': 'random'}\n",
    "\n",
    "metric = {'name': 'loss','goal': 'minimize'}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {'optimizer': {'values': ['adam', 'sgd']}}\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({'epochs': {'value': 4}})\n",
    "\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {'distribution': 'uniform','min': 0.0001,'max': 0.1},# a flat distribution between 0.0001 and 0.1\n",
    "    'batch_size': {'values': [16]}})# integers between 2 and 16\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: uufgku03\n",
      "Sweep URL: https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"pytorch project 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        dataset = ForeheadbboxDataset('data/Forehead/train', get_transform(train=True))\n",
    "        dataset_test = ForeheadbboxDataset('data/Forehead/train', get_transform(train=False))\n",
    "\n",
    "        # split the dataset in train and test set\n",
    "        indices = torch.randperm(len(dataset)).tolist()\n",
    "        dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "        dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "        dlTrain = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "        dlVal = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=utils.collate_fn)\n",
    "        oModel=build_network(numCls,NNType)\n",
    "\n",
    "        #freezing layers\n",
    "        for param in oModel.roi_heads.box_predictor.parameters():\n",
    "            param.requires_grad = True\n",
    "        if NNType=='Mask RCNN':\n",
    "            for param in oModel.roi_heads.mask_predictor.parameters():\n",
    "                param.requires_grad = True\n",
    "        oModel.to(runDevice)\n",
    "        optimizer = build_optimizer(oModel, config.optimizer, config.learning_rate)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "        for epoch in range(config.epochs):\n",
    "            # train for one epoch, printing every 10 iterations\n",
    "            logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
    "            # update the learning rate\n",
    "            lr_scheduler.step()\n",
    "            # evaluate on the test dataset\n",
    "            logger_eval=evaluate(oModel, dlVal, device=runDevice)#need to add meters from logger_eval to wandb logger, didn't get this far on CPU\n",
    "            wandb.log({\"loss\": logger.meters['loss'].avg, \"loss_classifier\":logger.meters['loss_classifier'].avg,\n",
    "                       \"loss_box_reg\":logger.meters['loss_box_reg'].avg, \"loss_objectness\":logger.meters['loss_objectness'].avg,\n",
    "                       \"loss_rpn_box_reg\":logger.meters['loss_rpn_box_reg'].avg,\"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: orr5wzc8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.04583988271857309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ForeheadDetection/wandb/run-20240713_172842-orr5wzc8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/imri/pytorch%20project%202/runs/orr5wzc8' target=\"_blank\">youthful-sweep-1</a></strong> to <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/imri/pytorch%20project%202/runs/orr5wzc8' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/runs/orr5wzc8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "    logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "    loss_dict = model(images, targets)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "    torch._assert(\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "    assert condition, message\n",
      "AssertionError: All bounding boxes should have positive height and width. Found invalid box [631.2718505859375, 656.6666259765625, 631.2718505859375, 656.6666259765625] for target at index 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5270cee33c744e82a5ea44d9717a664a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-1</strong> at: <a href='https://wandb.ai/imri/pytorch%20project%202/runs/orr5wzc8' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/runs/orr5wzc8</a><br/> View project at: <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240713_172842-orr5wzc8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run orr5wzc8 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "    logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "    loss_dict = model(images, targets)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "    torch._assert(\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "    assert condition, message\n",
      "AssertionError: All bounding boxes should have positive height and width. Found invalid box [631.2718505859375, 656.6666259765625, 631.2718505859375, 656.6666259765625] for target at index 0.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run orr5wzc8 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss_dict = model(images, targets)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch._assert(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert condition, message\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError: All bounding boxes should have positive height and width. Found invalid box [631.2718505859375, 656.6666259765625, 631.2718505859375, 656.6666259765625] for target at index 0.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2pzupuvk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.082562560793885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ForeheadDetection/wandb/run-20240713_172853-2pzupuvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/imri/pytorch%20project%202/runs/2pzupuvk' target=\"_blank\">atomic-sweep-2</a></strong> to <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/imri/pytorch%20project%202/runs/2pzupuvk' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/runs/2pzupuvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "    logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "    loss_dict = model(images, targets)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "    torch._assert(\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "    assert condition, message\n",
      "AssertionError: All bounding boxes should have positive height and width. Found invalid box [301.4781188964844, 285.0, 301.4781188964844, 285.0] for target at index 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e467f9babf248cabfb0bcfff9200707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-2</strong> at: <a href='https://wandb.ai/imri/pytorch%20project%202/runs/2pzupuvk' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/runs/2pzupuvk</a><br/> View project at: <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240713_172853-2pzupuvk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2pzupuvk errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "    logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "    loss_dict = model(images, targets)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "    torch._assert(\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "    assert condition, message\n",
      "AssertionError: All bounding boxes should have positive height and width. Found invalid box [301.4781188964844, 285.0, 301.4781188964844, 285.0] for target at index 0.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2pzupuvk errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss_dict = model(images, targets)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch._assert(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert condition, message\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError: All bounding boxes should have positive height and width. Found invalid box [301.4781188964844, 285.0, 301.4781188964844, 285.0] for target at index 0.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pv51s4b2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.07316290807165034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/ForeheadDetection/wandb/run-20240713_172902-pv51s4b2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/imri/pytorch%20project%202/runs/pv51s4b2' target=\"_blank\">worldly-sweep-3</a></strong> to <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/sweeps/uufgku03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/imri/pytorch%20project%202/runs/pv51s4b2' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/runs/pv51s4b2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "    logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "    loss_dict = model(images, targets)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "    torch._assert(\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "    assert condition, message\n",
      "AssertionError: All bounding boxes should have positive height and width. Found invalid box [547.9906005859375, 560.0, 547.9906005859375, 560.0] for target at index 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f397667b5e40cea3370eac0e2a423d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-sweep-3</strong> at: <a href='https://wandb.ai/imri/pytorch%20project%202/runs/pv51s4b2' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202/runs/pv51s4b2</a><br/> View project at: <a href='https://wandb.ai/imri/pytorch%20project%202' target=\"_blank\">https://wandb.ai/imri/pytorch%20project%202</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240713_172902-pv51s4b2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run pv51s4b2 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "    logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "    loss_dict = model(images, targets)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "    torch._assert(\n",
      "  File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "    assert condition, message\n",
      "AssertionError: All bounding boxes should have positive height and width. Found invalid box [547.9906005859375, 560.0, 547.9906005859375, 560.0] for target at index 0.\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run pv51s4b2 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_230326/2519608288.py\", line 41, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     logger=train_one_epoch(oModel, optimizer, dlTrain, runDevice, epoch, print_freq=10)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/ForeheadDetection/engine.py\", line 31, in train_one_epoch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss_dict = model(images, targets)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py\", line 95, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch._assert(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/ubuntu/.pyenv/versions/second_forehead_venv/lib/python3.12/site-packages/torch/__init__.py\", line 1561, in _assert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert condition, message\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError: All bounding boxes should have positive height and width. Found invalid box [547.9906005859375, 560.0, 547.9906005859375, 560.0] for target at index 0.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.000062  loss: 3.9202 (3.9202)  loss_classifier: 0.5476 (0.5476)  loss_box_reg: 0.0015 (0.0015)  loss_mask: 1.9977 (1.9977)  loss_objectness: 0.5192 (0.5192)  loss_rpn_box_reg: 0.8543 (0.8543)\n",
      "Epoch: [0]  [ 0/88]  eta: 0:20:17  lr: 0.000062  loss: 3.9202 (3.9202)  loss_classifier: 0.5476 (0.5476)  loss_box_reg: 0.0015 (0.0015)  loss_mask: 1.9977 (1.9977)  loss_objectness: 0.5192 (0.5192)  loss_rpn_box_reg: 0.8543 (0.8543)  time: 13.8344  data: 0.2463  max mem: 2793\n",
      "lr: 0.000120  loss: 3.9202 (3.9227)  loss_classifier: 0.5475 (0.5475)  loss_box_reg: 0.0015 (0.0015)  loss_mask: 1.9507 (1.9742)  loss_objectness: 0.5192 (0.5275)  loss_rpn_box_reg: 0.8543 (0.8720)\n",
      "lr: 0.000177  loss: 3.9202 (3.8976)  loss_classifier: 0.5475 (0.5472)  loss_box_reg: 0.0015 (0.0015)  loss_mask: 1.9507 (1.9260)  loss_objectness: 0.5192 (0.5008)  loss_rpn_box_reg: 0.8898 (0.9221)\n",
      "lr: 0.000235  loss: 3.9202 (3.9422)  loss_classifier: 0.5465 (0.5450)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.8688 (1.9117)  loss_objectness: 0.5192 (0.5067)  loss_rpn_box_reg: 0.8898 (0.9775)\n",
      "lr: 0.000292  loss: 3.9202 (3.8940)  loss_classifier: 0.5465 (0.5411)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.8688 (1.8836)  loss_objectness: 0.5192 (0.4703)  loss_rpn_box_reg: 1.0223 (0.9977)\n",
      "lr: 0.000349  loss: 3.8474 (3.7708)  loss_classifier: 0.5383 (0.5311)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.8296 (1.8103)  loss_objectness: 0.4476 (0.4481)  loss_rpn_box_reg: 0.8911 (0.9799)\n",
      "lr: 0.000407  loss: 3.8474 (3.6880)  loss_classifier: 0.5383 (0.5180)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.8296 (1.7311)  loss_objectness: 0.4767 (0.4522)  loss_rpn_box_reg: 1.0182 (0.9854)\n",
      "lr: 0.000464  loss: 3.7012 (3.5694)  loss_classifier: 0.5256 (0.5026)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.7712 (1.6309)  loss_objectness: 0.4641 (0.4537)  loss_rpn_box_reg: 0.9485 (0.9808)\n",
      "lr: 0.000522  loss: 3.7012 (3.4536)  loss_classifier: 0.5256 (0.4836)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.7712 (1.5209)  loss_objectness: 0.4767 (0.4677)  loss_rpn_box_reg: 0.9740 (0.9800)\n",
      "lr: 0.000579  loss: 3.1917 (3.3358)  loss_classifier: 0.4809 (0.4636)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.4439 (1.4136)  loss_objectness: 0.4641 (0.4673)  loss_rpn_box_reg: 0.9740 (0.9900)\n",
      "lr: 0.000637  loss: 3.1917 (3.2309)  loss_classifier: 0.4809 (0.4425)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.4439 (1.3061)  loss_objectness: 0.4767 (0.4927)  loss_rpn_box_reg: 0.9740 (0.9882)\n",
      "Epoch: [0]  [10/88]  eta: 0:03:00  lr: 0.000637  loss: 3.1917 (3.2309)  loss_classifier: 0.4809 (0.4425)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.4439 (1.3061)  loss_objectness: 0.4767 (0.4927)  loss_rpn_box_reg: 0.9740 (0.9882)  time: 2.3080  data: 0.0752  max mem: 2793\n",
      "lr: 0.000694  loss: 3.1546 (3.0933)  loss_classifier: 0.4396 (0.4214)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.2558 (1.2078)  loss_objectness: 0.4641 (0.4885)  loss_rpn_box_reg: 0.9704 (0.9742)\n",
      "lr: 0.000751  loss: 3.1546 (2.9844)  loss_classifier: 0.4396 (0.4006)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 1.2558 (1.1218)  loss_objectness: 0.4641 (0.4813)  loss_rpn_box_reg: 0.9740 (0.9794)\n",
      "lr: 0.000809  loss: 2.7386 (2.8826)  loss_classifier: 0.3949 (0.3804)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 0.9295 (1.0444)  loss_objectness: 0.4634 (0.4740)  loss_rpn_box_reg: 0.9740 (0.9824)\n",
      "lr: 0.000866  loss: 2.7386 (2.7928)  loss_classifier: 0.3949 (0.3610)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 0.9295 (0.9766)  loss_objectness: 0.4634 (0.4709)  loss_rpn_box_reg: 0.9894 (0.9829)\n",
      "lr: 0.000924  loss: 2.5275 (2.7139)  loss_classifier: 0.3313 (0.3433)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 0.6408 (0.9163)  loss_objectness: 0.4476 (0.4627)  loss_rpn_box_reg: 0.9894 (0.9902)\n",
      "lr: 0.000981  loss: 2.5275 (2.6327)  loss_classifier: 0.3313 (0.3263)  loss_box_reg: 0.0014 (0.0013)  loss_mask: 0.6408 (0.8629)  loss_objectness: 0.4476 (0.4600)  loss_rpn_box_reg: 0.9894 (0.9821)\n",
      "lr: 0.001038  loss: 2.2760 (2.5649)  loss_classifier: 0.2837 (0.3105)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 0.4479 (0.8151)  loss_objectness: 0.4424 (0.4564)  loss_rpn_box_reg: 0.9740 (0.9815)\n",
      "lr: 0.001096  loss: 2.2760 (2.5104)  loss_classifier: 0.2837 (0.2962)  loss_box_reg: 0.0014 (0.0014)  loss_mask: 0.4479 (0.7724)  loss_objectness: 0.4476 (0.4620)  loss_rpn_box_reg: 0.9740 (0.9785)\n",
      "lr: 0.001153  loss: 2.1819 (2.4683)  loss_classifier: 0.2319 (0.2828)  loss_box_reg: 0.0013 (0.0014)  loss_mask: 0.2314 (0.7338)  loss_objectness: 0.4476 (0.4621)  loss_rpn_box_reg: 0.9740 (0.9883)\n",
      "lr: 0.001211  loss: 1.6777 (2.4261)  loss_classifier: 0.1893 (0.2707)  loss_box_reg: 0.0013 (0.0013)  loss_mask: 0.1261 (0.6990)  loss_objectness: 0.4476 (0.4628)  loss_rpn_box_reg: 0.9894 (0.9923)\n",
      "Epoch: [0]  [20/88]  eta: 0:01:58  lr: 0.001211  loss: 1.6777 (2.4261)  loss_classifier: 0.1893 (0.2707)  loss_box_reg: 0.0013 (0.0013)  loss_mask: 0.1261 (0.6990)  loss_objectness: 0.4476 (0.4628)  loss_rpn_box_reg: 0.9894 (0.9923)  time: 1.1370  data: 0.0495  max mem: 2793\n",
      "lr: 0.001268  loss: 1.6691 (2.3743)  loss_classifier: 0.1508 (0.2593)  loss_box_reg: 0.0013 (0.0013)  loss_mask: 0.0901 (0.6672)  loss_objectness: 0.4424 (0.4567)  loss_rpn_box_reg: 0.9894 (0.9897)\n",
      "lr: 0.001326  loss: 1.6212 (2.3415)  loss_classifier: 0.1176 (0.2489)  loss_box_reg: 0.0013 (0.0013)  loss_mask: 0.0381 (0.6382)  loss_objectness: 0.4272 (0.4533)  loss_rpn_box_reg: 0.9894 (0.9997)\n",
      "lr: 0.001383  loss: 1.5823 (2.3028)  loss_classifier: 0.0898 (0.2393)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0280 (0.6117)  loss_objectness: 0.4219 (0.4520)  loss_rpn_box_reg: 0.9740 (0.9985)\n",
      "lr: 0.001440  loss: 1.5797 (2.2722)  loss_classifier: 0.0769 (0.2304)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0118 (0.5872)  loss_objectness: 0.4272 (0.4579)  loss_rpn_box_reg: 0.9710 (0.9954)\n",
      "lr: 0.001498  loss: 1.5591 (2.2287)  loss_classifier: 0.0552 (0.2221)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0076 (0.5646)  loss_objectness: 0.4272 (0.4491)  loss_rpn_box_reg: 0.9710 (0.9916)\n",
      "lr: 0.001555  loss: 1.5591 (2.2063)  loss_classifier: 0.0424 (0.2145)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0031 (0.5437)  loss_objectness: 0.4272 (0.4501)  loss_rpn_box_reg: 0.9710 (0.9966)\n",
      "lr: 0.001613  loss: 1.5382 (2.1784)  loss_classifier: 0.0372 (0.2074)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0027 (0.5243)  loss_objectness: 0.4219 (0.4482)  loss_rpn_box_reg: 0.9740 (0.9973)\n",
      "lr: 0.001670  loss: 1.5357 (2.1549)  loss_classifier: 0.0293 (0.2007)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0020 (0.5062)  loss_objectness: 0.4219 (0.4510)  loss_rpn_box_reg: 0.9710 (0.9957)\n",
      "lr: 0.001727  loss: 1.5293 (2.1293)  loss_classifier: 0.0273 (0.1945)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0016 (0.4894)  loss_objectness: 0.4219 (0.4519)  loss_rpn_box_reg: 0.9704 (0.9922)\n",
      "lr: 0.001785  loss: 1.5286 (2.0996)  loss_classifier: 0.0207 (0.1886)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0009 (0.4736)  loss_objectness: 0.4177 (0.4460)  loss_rpn_box_reg: 0.9702 (0.9901)\n",
      "Epoch: [0]  [30/88]  eta: 0:01:33  lr: 0.001785  loss: 1.5286 (2.0996)  loss_classifier: 0.0207 (0.1886)  loss_box_reg: 0.0012 (0.0013)  loss_mask: 0.0009 (0.4736)  loss_objectness: 0.4177 (0.4460)  loss_rpn_box_reg: 0.9702 (0.9901)  time: 1.2175  data: 0.0761  max mem: 2793\n",
      "lr: 0.001842  loss: 1.4970 (2.0716)  loss_classifier: 0.0206 (0.1832)  loss_box_reg: 0.0011 (0.0013)  loss_mask: 0.0005 (0.4588)  loss_objectness: 0.3967 (0.4409)  loss_rpn_box_reg: 0.9702 (0.9875)\n",
      "lr: 0.001900  loss: 1.4598 (2.0531)  loss_classifier: 0.0182 (0.1780)  loss_box_reg: 0.0011 (0.0013)  loss_mask: 0.0003 (0.4449)  loss_objectness: 0.3967 (0.4391)  loss_rpn_box_reg: 0.9702 (0.9899)\n",
      "lr: 0.001957  loss: 1.4273 (2.0265)  loss_classifier: 0.0167 (0.1732)  loss_box_reg: 0.0011 (0.0012)  loss_mask: 0.0003 (0.4318)  loss_objectness: 0.3967 (0.4325)  loss_rpn_box_reg: 0.9506 (0.9879)\n",
      "lr: 0.002014  loss: 1.4273 (2.0111)  loss_classifier: 0.0162 (0.1686)  loss_box_reg: 0.0011 (0.0012)  loss_mask: 0.0001 (0.4195)  loss_objectness: 0.3967 (0.4323)  loss_rpn_box_reg: 0.9506 (0.9895)\n",
      "lr: 0.002072  loss: 1.4132 (1.9929)  loss_classifier: 0.0158 (0.1643)  loss_box_reg: 0.0011 (0.0012)  loss_mask: 0.0001 (0.4078)  loss_objectness: 0.3967 (0.4311)  loss_rpn_box_reg: 0.9506 (0.9884)\n",
      "lr: 0.002129  loss: 1.4132 (1.9699)  loss_classifier: 0.0147 (0.1602)  loss_box_reg: 0.0011 (0.0012)  loss_mask: 0.0001 (0.3968)  loss_objectness: 0.3952 (0.4265)  loss_rpn_box_reg: 0.9506 (0.9851)\n",
      "lr: 0.002187  loss: 1.4116 (1.9528)  loss_classifier: 0.0141 (0.1563)  loss_box_reg: 0.0011 (0.0012)  loss_mask: 0.0001 (0.3863)  loss_objectness: 0.3895 (0.4237)  loss_rpn_box_reg: 0.9506 (0.9853)\n",
      "lr: 0.002244  loss: 1.4116 (1.9429)  loss_classifier: 0.0137 (0.1527)  loss_box_reg: 0.0010 (0.0012)  loss_mask: 0.0001 (0.3764)  loss_objectness: 0.3895 (0.4247)  loss_rpn_box_reg: 0.9520 (0.9878)\n",
      "lr: 0.002302  loss: 1.3848 (1.9277)  loss_classifier: 0.0136 (0.1492)  loss_box_reg: 0.0010 (0.0012)  loss_mask: 0.0001 (0.3670)  loss_objectness: 0.3895 (0.4258)  loss_rpn_box_reg: 0.9506 (0.9845)\n",
      "lr: 0.002359  loss: 1.3848 (1.9205)  loss_classifier: 0.0136 (0.1459)  loss_box_reg: 0.0010 (0.0012)  loss_mask: 0.0001 (0.3581)  loss_objectness: 0.3895 (0.4306)  loss_rpn_box_reg: 0.9506 (0.9847)\n",
      "Epoch: [0]  [40/88]  eta: 0:01:13  lr: 0.002359  loss: 1.3848 (1.9205)  loss_classifier: 0.0136 (0.1459)  loss_box_reg: 0.0010 (0.0012)  loss_mask: 0.0001 (0.3581)  loss_objectness: 0.3895 (0.4306)  loss_rpn_box_reg: 0.9506 (0.9847)  time: 1.2988  data: 0.1093  max mem: 2793\n",
      "lr: 0.002416  loss: 1.4116 (1.9178)  loss_classifier: 0.0136 (0.1428)  loss_box_reg: 0.0010 (0.0012)  loss_mask: 0.0001 (0.3496)  loss_objectness: 0.3967 (0.4353)  loss_rpn_box_reg: 0.9520 (0.9889)\n",
      "lr: 0.002474  loss: 1.4116 (1.9100)  loss_classifier: 0.0136 (0.1398)  loss_box_reg: 0.0009 (0.0012)  loss_mask: 0.0001 (0.3414)  loss_objectness: 0.4219 (0.4378)  loss_rpn_box_reg: 0.9520 (0.9898)\n",
      "lr: 0.002531  loss: 1.4273 (1.8994)  loss_classifier: 0.0136 (0.1370)  loss_box_reg: 0.0009 (0.0012)  loss_mask: 0.0000 (0.3337)  loss_objectness: 0.4273 (0.4408)  loss_rpn_box_reg: 0.9506 (0.9867)\n",
      "lr: 0.002589  loss: 1.3848 (1.8817)  loss_classifier: 0.0135 (0.1343)  loss_box_reg: 0.0009 (0.0011)  loss_mask: 0.0000 (0.3263)  loss_objectness: 0.3967 (0.4353)  loss_rpn_box_reg: 0.9506 (0.9847)\n",
      "lr: 0.002646  loss: 1.3848 (1.8702)  loss_classifier: 0.0135 (0.1316)  loss_box_reg: 0.0009 (0.0011)  loss_mask: 0.0000 (0.3192)  loss_objectness: 0.4079 (0.4347)  loss_rpn_box_reg: 0.9506 (0.9835)\n",
      "lr: 0.002703  loss: 1.3848 (1.8634)  loss_classifier: 0.0135 (0.1291)  loss_box_reg: 0.0009 (0.0011)  loss_mask: 0.0000 (0.3124)  loss_objectness: 0.4079 (0.4359)  loss_rpn_box_reg: 0.9506 (0.9849)\n",
      "lr: 0.002761  loss: 1.3848 (1.8548)  loss_classifier: 0.0135 (0.1268)  loss_box_reg: 0.0008 (0.0011)  loss_mask: 0.0000 (0.3059)  loss_objectness: 0.4273 (0.4379)  loss_rpn_box_reg: 0.9310 (0.9831)\n",
      "lr: 0.002818  loss: 1.3848 (1.8480)  loss_classifier: 0.0135 (0.1245)  loss_box_reg: 0.0008 (0.0011)  loss_mask: 0.0000 (0.2996)  loss_objectness: 0.4273 (0.4378)  loss_rpn_box_reg: 0.9310 (0.9850)\n",
      "lr: 0.002876  loss: 1.3561 (1.8374)  loss_classifier: 0.0135 (0.1223)  loss_box_reg: 0.0008 (0.0011)  loss_mask: 0.0000 (0.2936)  loss_objectness: 0.4108 (0.4372)  loss_rpn_box_reg: 0.9310 (0.9832)\n",
      "lr: 0.002933  loss: 1.4409 (1.8298)  loss_classifier: 0.0135 (0.1201)  loss_box_reg: 0.0008 (0.0011)  loss_mask: 0.0000 (0.2879)  loss_objectness: 0.4273 (0.4389)  loss_rpn_box_reg: 0.9310 (0.9817)\n",
      "Epoch: [0]  [50/88]  eta: 0:00:55  lr: 0.002933  loss: 1.4409 (1.8298)  loss_classifier: 0.0135 (0.1201)  loss_box_reg: 0.0008 (0.0011)  loss_mask: 0.0000 (0.2879)  loss_objectness: 0.4273 (0.4389)  loss_rpn_box_reg: 0.9310 (0.9817)  time: 1.2666  data: 0.1018  max mem: 2793\n",
      "lr: 0.002991  loss: 1.4457 (1.8232)  loss_classifier: 0.0135 (0.1181)  loss_box_reg: 0.0007 (0.0011)  loss_mask: 0.0000 (0.2823)  loss_objectness: 0.4308 (0.4407)  loss_rpn_box_reg: 0.9421 (0.9810)\n",
      "lr: 0.003048  loss: 1.4409 (1.8156)  loss_classifier: 0.0138 (0.1162)  loss_box_reg: 0.0007 (0.0011)  loss_mask: 0.0000 (0.2770)  loss_objectness: 0.4592 (0.4411)  loss_rpn_box_reg: 0.9421 (0.9802)\n",
      "lr: 0.003105  loss: 1.4457 (1.8100)  loss_classifier: 0.0140 (0.1143)  loss_box_reg: 0.0007 (0.0011)  loss_mask: 0.0000 (0.2719)  loss_objectness: 0.4653 (0.4429)  loss_rpn_box_reg: 0.9432 (0.9798)\n",
      "lr: 0.003163  loss: 1.4409 (1.7996)  loss_classifier: 0.0140 (0.1125)  loss_box_reg: 0.0007 (0.0010)  loss_mask: 0.0000 (0.2669)  loss_objectness: 0.4653 (0.4394)  loss_rpn_box_reg: 0.9432 (0.9798)\n",
      "lr: 0.003220  loss: 1.4409 (1.7931)  loss_classifier: 0.0142 (0.1107)  loss_box_reg: 0.0006 (0.0010)  loss_mask: 0.0000 (0.2622)  loss_objectness: 0.4672 (0.4400)  loss_rpn_box_reg: 0.9432 (0.9792)\n",
      "lr: 0.003278  loss: 1.4409 (1.7856)  loss_classifier: 0.0142 (0.1090)  loss_box_reg: 0.0006 (0.0010)  loss_mask: 0.0000 (0.2576)  loss_objectness: 0.4690 (0.4405)  loss_rpn_box_reg: 0.9432 (0.9775)\n",
      "lr: 0.003335  loss: 1.4457 (1.7802)  loss_classifier: 0.0142 (0.1074)  loss_box_reg: 0.0006 (0.0010)  loss_mask: 0.0000 (0.2531)  loss_objectness: 0.4690 (0.4394)  loss_rpn_box_reg: 0.9432 (0.9793)\n",
      "lr: 0.003392  loss: 1.4409 (1.7719)  loss_classifier: 0.0142 (0.1058)  loss_box_reg: 0.0005 (0.0010)  loss_mask: 0.0000 (0.2488)  loss_objectness: 0.4690 (0.4353)  loss_rpn_box_reg: 0.9432 (0.9809)\n",
      "lr: 0.003450  loss: 1.4409 (1.7663)  loss_classifier: 0.0142 (0.1042)  loss_box_reg: 0.0005 (0.0010)  loss_mask: 0.0000 (0.2447)  loss_objectness: 0.4690 (0.4354)  loss_rpn_box_reg: 0.9434 (0.9810)\n",
      "lr: 0.003507  loss: 1.4389 (1.7601)  loss_classifier: 0.0140 (0.1027)  loss_box_reg: 0.0005 (0.0010)  loss_mask: 0.0000 (0.2407)  loss_objectness: 0.4592 (0.4323)  loss_rpn_box_reg: 0.9434 (0.9834)\n",
      "Epoch: [0]  [60/88]  eta: 0:00:40  lr: 0.003507  loss: 1.4389 (1.7601)  loss_classifier: 0.0140 (0.1027)  loss_box_reg: 0.0005 (0.0010)  loss_mask: 0.0000 (0.2407)  loss_objectness: 0.4592 (0.4323)  loss_rpn_box_reg: 0.9434 (0.9834)  time: 1.2381  data: 0.0880  max mem: 2793\n",
      "lr: 0.003565  loss: 1.4315 (1.7531)  loss_classifier: 0.0138 (0.1013)  loss_box_reg: 0.0005 (0.0010)  loss_mask: 0.0000 (0.2368)  loss_objectness: 0.4368 (0.4319)  loss_rpn_box_reg: 0.9432 (0.9822)\n",
      "lr: 0.003622  loss: 1.4183 (1.7464)  loss_classifier: 0.0138 (0.0999)  loss_box_reg: 0.0005 (0.0010)  loss_mask: 0.0000 (0.2330)  loss_objectness: 0.4308 (0.4281)  loss_rpn_box_reg: 0.9432 (0.9844)\n",
      "lr: 0.003679  loss: 1.3903 (1.7391)  loss_classifier: 0.0136 (0.0986)  loss_box_reg: 0.0005 (0.0010)  loss_mask: 0.0000 (0.2294)  loss_objectness: 0.4108 (0.4275)  loss_rpn_box_reg: 0.9432 (0.9827)\n",
      "lr: 0.003737  loss: 1.4183 (1.7353)  loss_classifier: 0.0136 (0.0972)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2259)  loss_objectness: 0.4108 (0.4270)  loss_rpn_box_reg: 0.9434 (0.9842)\n",
      "lr: 0.003794  loss: 1.4183 (1.7301)  loss_classifier: 0.0136 (0.0960)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2225)  loss_objectness: 0.4308 (0.4296)  loss_rpn_box_reg: 0.9434 (0.9811)\n",
      "lr: 0.003852  loss: 1.3936 (1.7246)  loss_classifier: 0.0135 (0.0947)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2191)  loss_objectness: 0.4108 (0.4290)  loss_rpn_box_reg: 0.9434 (0.9808)\n",
      "lr: 0.003909  loss: 1.3903 (1.7178)  loss_classifier: 0.0135 (0.0935)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2159)  loss_objectness: 0.4050 (0.4284)  loss_rpn_box_reg: 0.9434 (0.9790)\n",
      "lr: 0.003967  loss: 1.3692 (1.7102)  loss_classifier: 0.0135 (0.0924)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2128)  loss_objectness: 0.3966 (0.4265)  loss_rpn_box_reg: 0.9432 (0.9776)\n",
      "lr: 0.004024  loss: 1.3692 (1.7044)  loss_classifier: 0.0135 (0.0913)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2097)  loss_objectness: 0.3903 (0.4238)  loss_rpn_box_reg: 0.9434 (0.9787)\n",
      "lr: 0.004081  loss: 1.3594 (1.6954)  loss_classifier: 0.0134 (0.0901)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2068)  loss_objectness: 0.3893 (0.4212)  loss_rpn_box_reg: 0.9434 (0.9764)\n",
      "Epoch: [0]  [70/88]  eta: 0:00:25  lr: 0.004081  loss: 1.3594 (1.6954)  loss_classifier: 0.0134 (0.0901)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2068)  loss_objectness: 0.3893 (0.4212)  loss_rpn_box_reg: 0.9434 (0.9764)  time: 1.2242  data: 0.0830  max mem: 2793\n",
      "lr: 0.004139  loss: 1.3594 (1.6913)  loss_classifier: 0.0134 (0.0891)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2039)  loss_objectness: 0.3892 (0.4205)  loss_rpn_box_reg: 0.9560 (0.9769)\n",
      "lr: 0.004196  loss: 1.3292 (1.6854)  loss_classifier: 0.0133 (0.0880)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.2011)  loss_objectness: 0.3785 (0.4195)  loss_rpn_box_reg: 0.9560 (0.9758)\n",
      "lr: 0.004254  loss: 1.3251 (1.6798)  loss_classifier: 0.0133 (0.0870)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.1984)  loss_objectness: 0.3703 (0.4176)  loss_rpn_box_reg: 0.9560 (0.9759)\n",
      "lr: 0.004311  loss: 1.3251 (1.6749)  loss_classifier: 0.0133 (0.0860)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.1958)  loss_objectness: 0.3703 (0.4164)  loss_rpn_box_reg: 0.9560 (0.9759)\n",
      "lr: 0.004368  loss: 1.3190 (1.6702)  loss_classifier: 0.0133 (0.0851)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.1932)  loss_objectness: 0.3485 (0.4144)  loss_rpn_box_reg: 0.9703 (0.9767)\n",
      "lr: 0.004426  loss: 1.3190 (1.6665)  loss_classifier: 0.0133 (0.0842)  loss_box_reg: 0.0004 (0.0009)  loss_mask: 0.0000 (0.1907)  loss_objectness: 0.3485 (0.4143)  loss_rpn_box_reg: 0.9703 (0.9765)\n",
      "lr: 0.004483  loss: 1.3190 (1.6630)  loss_classifier: 0.0133 (0.0832)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1882)  loss_objectness: 0.3485 (0.4143)  loss_rpn_box_reg: 0.9703 (0.9764)\n",
      "lr: 0.004541  loss: 1.3190 (1.6585)  loss_classifier: 0.0133 (0.0824)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1858)  loss_objectness: 0.3485 (0.4134)  loss_rpn_box_reg: 0.9616 (0.9761)\n",
      "lr: 0.004598  loss: 1.3190 (1.6596)  loss_classifier: 0.0133 (0.0815)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1835)  loss_objectness: 0.3485 (0.4149)  loss_rpn_box_reg: 0.9616 (0.9788)\n",
      "lr: 0.004656  loss: 1.3190 (1.6565)  loss_classifier: 0.0133 (0.0807)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1813)  loss_objectness: 0.3485 (0.4137)  loss_rpn_box_reg: 0.9616 (0.9801)\n",
      "Epoch: [0]  [80/88]  eta: 0:00:11  lr: 0.004656  loss: 1.3190 (1.6565)  loss_classifier: 0.0133 (0.0807)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1813)  loss_objectness: 0.3485 (0.4137)  loss_rpn_box_reg: 0.9616 (0.9801)  time: 1.2189  data: 0.0826  max mem: 2793\n",
      "lr: 0.004713  loss: 1.3190 (1.6558)  loss_classifier: 0.0133 (0.0799)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1790)  loss_objectness: 0.3485 (0.4156)  loss_rpn_box_reg: 0.9703 (0.9804)\n",
      "lr: 0.004770  loss: 1.3127 (1.6506)  loss_classifier: 0.0133 (0.0791)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1769)  loss_objectness: 0.3485 (0.4145)  loss_rpn_box_reg: 0.9616 (0.9794)\n",
      "lr: 0.004828  loss: 1.3127 (1.6458)  loss_classifier: 0.0135 (0.0783)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1748)  loss_objectness: 0.3478 (0.4131)  loss_rpn_box_reg: 0.9616 (0.9788)\n",
      "lr: 0.004885  loss: 1.3127 (1.6429)  loss_classifier: 0.0135 (0.0775)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1727)  loss_objectness: 0.3478 (0.4136)  loss_rpn_box_reg: 0.9560 (0.9782)\n",
      "lr: 0.004943  loss: 1.3127 (1.6414)  loss_classifier: 0.0135 (0.0768)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1707)  loss_objectness: 0.3478 (0.4151)  loss_rpn_box_reg: 0.9616 (0.9781)\n",
      "lr: 0.005000  loss: 1.3092 (1.6360)  loss_classifier: 0.0135 (0.0760)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1688)  loss_objectness: 0.3251 (0.4139)  loss_rpn_box_reg: 0.9616 (0.9765)\n",
      "lr: 0.005000  loss: 1.3092 (1.6291)  loss_classifier: 0.0133 (0.0753)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1668)  loss_objectness: 0.3222 (0.4116)  loss_rpn_box_reg: 0.9616 (0.9745)\n",
      "Epoch: [0]  [87/88]  eta: 0:00:01  lr: 0.005000  loss: 1.3092 (1.6291)  loss_classifier: 0.0133 (0.0753)  loss_box_reg: 0.0004 (0.0008)  loss_mask: 0.0000 (0.1668)  loss_objectness: 0.3222 (0.4116)  loss_rpn_box_reg: 0.9616 (0.9745)  time: 1.4760  data: 0.0772  max mem: 2914\n",
      "Epoch: [0] Total time: 0:02:05 (1.4263 s / it)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# evaluate on the test dataset\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     b\u001b[38;5;241m=\u001b[39m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlVal_tst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrunDevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms it!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\micromamba\\envs\\TechnionAiProg\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\Documents\\Imri\\ai\\project 2\\engine.py:103\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m    101\u001b[0m res \u001b[38;5;241m=\u001b[39m {target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]: output \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)}\n\u001b[0;32m    102\u001b[0m evaluator_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 103\u001b[0m \u001b[43mcoco_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m evaluator_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m evaluator_time\n\u001b[0;32m    105\u001b[0m metric_logger\u001b[38;5;241m.\u001b[39mupdate(model_time\u001b[38;5;241m=\u001b[39mmodel_time, evaluator_time\u001b[38;5;241m=\u001b[39mevaluator_time)\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\Documents\\Imri\\ai\\project 2\\coco_eval.py:41\u001b[0m, in \u001b[0;36mCocoEvaluator.update\u001b[1;34m(self, predictions)\u001b[0m\n\u001b[0;32m     39\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39mcocoDt \u001b[38;5;241m=\u001b[39m coco_dt\n\u001b[0;32m     40\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mimgIds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img_ids)\n\u001b[1;32m---> 41\u001b[0m img_ids, eval_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_imgs[iou_type]\u001b[38;5;241m.\u001b[39mappend(eval_imgs)\n",
      "File \u001b[1;32mc:\\Users\\i.fiedelman\\Documents\\Imri\\ai\\project 2\\coco_eval.py:200\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(imgs)\u001b[0m\n\u001b[0;32m    198\u001b[0m imgs\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m    199\u001b[0m img_ids \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mimgIds \u001b[38;5;66;03m#new\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m eval_imgs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevalImgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#new\u001b[39;00m\n\u001b[0;32m    201\u001b[0m reshaped_eval_imgs \u001b[38;5;241m=\u001b[39m eval_imgs_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(imgs\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mareaRng), \u001b[38;5;28mlen\u001b[39m(img_ids)) \u001b[38;5;66;03m#new\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_ids, reshaped_eval_imgs\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# # # training block, nothing is truely defined here with actual thought,everything is copied from tutorial,\n",
    "# # # from here wanDB and maybe gridsearch/optuna should be defined and hyper parameters searched, \n",
    "# # # maybe different optimizer and learning rate schedular, all weight are unfrozen\n",
    "\n",
    "# # # move model to the right device\n",
    "# oModel.to(runDevice)\n",
    "\n",
    "# # # construct an optimizer\n",
    "# params = [p for p in oModel.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     params,\n",
    "#     lr=0.005,\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0.0005\n",
    "# )\n",
    "\n",
    "# # and a learning rate scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=3,\n",
    "#     gamma=0.1\n",
    "# )\n",
    "\n",
    "# # # let's train it just for 2 epochs\n",
    "# num_epochs = 1\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # train for one epoch, printing every 10 iterations\n",
    "#     a=train_one_epoch(oModel, optimizer, dlTrain_tst, runDevice, epoch, print_freq=10)\n",
    "#     torch.save(oModel, 'BestModel.pt')\n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "#     # evaluate on the test dataset\n",
    "#     b=evaluate(oModel, dlVal, device=runDevice)\n",
    "\n",
    "# print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# oModel.eval()\n",
    "# oModel.to(runDevice)\n",
    "# a=evaluate(oModel, data_loader_test, device=runDevice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
